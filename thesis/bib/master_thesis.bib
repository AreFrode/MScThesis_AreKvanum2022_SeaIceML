@Article{Palerme2021,
  author    = {Cyril Palerme and Malte Müller},
  title     = {Calibration of sea ice drift forecasts using random forest algorithms},
  year      = {2021},
  month     = {aug},
  number    = {8},
  pages     = {3989--4004},
  volume    = {15},
  doi       = {10.5194/tc-15-3989-2021},
  file      = {:palerme2021.pdf:PDF},
  publisher = {Copernicus {GmbH}},
}

@Article{Fritzner2020,
  author    = {Sindre Fritzner and Rune Graversen and Kai H. Christensen},
  title     = {Assessment of High-Resolution Dynamical and Machine Learning Models for Prediction of Sea Ice Concentration in a Regional Application},
  year      = {2020},
  month     = oct,
  note      = {Neural Networks for predicting Sea-Ice concentration are only slightly more accurate than persistence forecasting for short-term predictions.},
  number    = {11},
  volume    = {125},
  comment   = {*Read*},
  doi       = {10.1029/2020jc016277},
  file      = {:fritzner2020.pdf:PDF},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Soenderby2020,
  author        = {Casper Kaae Sønderby and Lasse Espeholt and Jonathan Heek and Mostafa Dehghani and Avital Oliver and Tim Salimans and Shreya Agrawal and Jason Hickey and Nal Kalchbrenner},
  title         = {MetNet: A Neural Weather Model for Precipitation Forecasting},
  year          = {2020},
  month         = mar,
  abstract      = {Weather forecasting is a long standing scientific challenge with direct social and economic impact. The task is suitable for deep neural networks due to vast amounts of continuously collected data and a rich spatial and temporal structure that presents long range dependencies. We introduce MetNet, a neural network that forecasts precipitation up to 8 hours into the future at the high spatial resolution of 1 km$^2$ and at the temporal resolution of 2 minutes with a latency in the order of seconds. MetNet takes as input radar and satellite data and forecast lead time and produces a probabilistic precipitation map. The architecture uses axial self-attention to aggregate the global context from a large input patch corresponding to a million square kilometers. We evaluate the performance of MetNet at various precipitation thresholds and find that MetNet outperforms Numerical Weather Prediction at forecasts of up to 7 to 8 hours on the scale of the continental United States.},
  archiveprefix = {arXiv},
  eprint        = {2003.12140},
  file          = {:http\://arxiv.org/pdf/2003.12140v2:PDF},
  keywords      = {cs.LG, physics.ao-ph, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Agrawal2019,
  author        = {Shreya Agrawal and Luke Barrington and Carla Bromberg and John Burge and Cenk Gazen and Jason Hickey},
  title         = {Machine Learning for Precipitation Nowcasting from Radar Images},
  year          = {2019},
  month         = dec,
  abstract      = {High-resolution nowcasting is an essential tool needed for effective adaptation to climate change, particularly for extreme weather. As Deep Learning (DL) techniques have shown dramatic promise in many domains, including the geosciences, we present an application of DL to the problem of precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1 hour) predictions of precipitation. We treat forecasting as an image-to-image translation problem and leverage the power of the ubiquitous UNET convolutional neural network. We find this performs favorably when compared to three commonly used models: optical flow, persistence and NOAA's numerical one-hour HRRR nowcasting prediction.},
  archiveprefix = {arXiv},
  eprint        = {1912.12132},
  file          = {:http\://arxiv.org/pdf/1912.12132v1:PDF},
  keywords      = {cs.CV, cs.LG, stat.ML},
  primaryclass  = {cs.CV},
}

@Article{Ravuri2021,
  author    = {Suman Ravuri and Karel Lenc and Matthew Willson and Dmitry Kangin and Remi Lam and Piotr Mirowski and Megan Fitzsimons and Maria Athanassiadou and Sheleem Kashem and Sam Madge and Rachel Prudden and Amol Mandhane and Aidan Clark and Andrew Brock and Karen Simonyan and Raia Hadsell and Niall Robinson and Ellen Clancy and Alberto Arribas and Shakir Mohamed},
  journal   = {Nature},
  title     = {Skilful precipitation nowcasting using deep generative models of radar},
  year      = {2021},
  month     = {sep},
  number    = {7878},
  pages     = {672--677},
  volume    = {597},
  doi       = {10.1038/s41586-021-03854-z},
  file      = {:ravuri2021.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Schultz2021,
  author    = {M. G. Schultz and C. Betancourt and B. Gong and F. Kleinert and M. Langguth and L. H. Leufen and A. Mozaffari and S. Stadtler},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title     = {Can deep learning beat numerical weather prediction?},
  year      = {2021},
  month     = {feb},
  number    = {2194},
  pages     = {20200097},
  volume    = {379},
  comment   = {*READ*

Section 4 covers a lot of interesting problems surrounding meteorological data, and supplies papers which discusses these "shortcomings" in a ML context. Great paper to use for finding articles which discusses individual problems in-depth

Section 5 touch upon data preparation and model evaluation, especially data splitting with regards to auto-correlation. Also, sources discussing more suitable verification metrics than MSE due to the spatio-temporal correlation of meteorological data are referred to.},
  doi       = {10.1098/rsta.2020.0097},
  file      = {:schultz2021.pdf:PDF},
  publisher = {The Royal Society},
}

@Article{Espeholt2021,
  author        = {Lasse Espeholt and Shreya Agrawal and Casper Sønderby and Manoj Kumar and Jonathan Heek and Carla Bromberg and Cenk Gazen and Jason Hickey and Aaron Bell and Nal Kalchbrenner},
  title         = {Skillful Twelve Hour Precipitation Forecasts using Large Context Neural Networks},
  year          = {2021},
  month         = nov,
  note          = {Met-Net 2},
  abstract      = {The problem of forecasting weather has been scientifically studied for centuries due to its high impact on human lives, transportation, food production and energy management, among others. Current operational forecasting models are based on physics and use supercomputers to simulate the atmosphere to make forecasts hours and days in advance. Better physics-based forecasts require improvements in the models themselves, which can be a substantial scientific challenge, as well as improvements in the underlying resolution, which can be computationally prohibitive. An emerging class of weather models based on neural networks represents a paradigm shift in weather forecasting: the models learn the required transformations from data instead of relying on hand-coded physics and are computationally efficient. For neural models, however, each additional hour of lead time poses a substantial challenge as it requires capturing ever larger spatial contexts and increases the uncertainty of the prediction. In this work, we present a neural network that is capable of large-scale precipitation forecasting up to twelve hours ahead and, starting from the same atmospheric state, the model achieves greater skill than the state-of-the-art physics-based models HRRR and HREF that currently operate in the Continental United States. Interpretability analyses reinforce the observation that the model learns to emulate advanced physics principles. These results represent a substantial step towards establishing a new paradigm of efficient forecasting with neural networks.},
  archiveprefix = {arXiv},
  eprint        = {2111.07470},
  file          = {:http\://arxiv.org/pdf/2111.07470v1:PDF;:espeholt2021.pdf:PDF},
  keywords      = {cs.LG, physics.ao-ph},
  primaryclass  = {cs.LG},
}

@Article{Chantry2021,
  author    = {Matthew Chantry and Hannah Christensen and Peter Dueben and Tim Palmer},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title     = {Opportunities and challenges for machine learning in weather and climate modelling: hard, medium and soft {AI}},
  year      = {2021},
  month     = {feb},
  number    = {2194},
  pages     = {20200083},
  volume    = {379},
  doi       = {10.1098/rsta.2020.0083},
  file      = {:chantry2021.pdf:PDF},
  publisher = {The Royal Society},
}

@Article{Andersson2021,
  author    = {Tom R. Andersson and J. Scott Hosking and Mar{\'{\i}}a P{\'{e}}rez-Ortiz and Brooks Paige and Andrew Elliott and Chris Russell and Stephen Law and Daniel C. Jones and Jeremy Wilkinson and Tony Phillips and James Byrne and Steffen Tietsche and Beena Balan Sarojini and Eduardo Blanchard-Wrigglesworth and Yevgeny Aksenov and Rod Downie and Emily Shuckburgh},
  journal   = {Nature Communications},
  title     = {Seasonal Arctic sea ice forecasting with probabilistic deep learning},
  year      = {2021},
  month     = {aug},
  number    = {1},
  volume    = {12},
  comment   = {Unet for seasonal prediction},
  doi       = {10.1038/s41467-021-25257-4},
  file      = {:Andersson2021.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Zhu2017,
  author    = {Xiao Xiang Zhu and Devis Tuia and Lichao Mou and Gui-Song Xia and Liangpei Zhang and Feng Xu and Friedrich Fraundorfer},
  journal   = {{IEEE} Geoscience and Remote Sensing Magazine},
  title     = {Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources},
  year      = {2017},
  month     = {dec},
  number    = {4},
  pages     = {8--36},
  volume    = {5},
  comment   = {CNN implementation for recognizing spatial features in satellite images},
  doi       = {10.1109/mgrs.2017.2762307},
  file      = {:Zhu2017.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Shi2015,
  author        = {Xingjian Shi and Zhourong Chen and Hao Wang and Dit-Yan Yeung and Wai-kin Wong and Wang-chun Woo},
  title         = {Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting},
  year          = {2015},
  month         = jun,
  abstract      = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
  archiveprefix = {arXiv},
  eprint        = {1506.04214},
  file          = {:http\://arxiv.org/pdf/1506.04214v2:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Wandel2020,
  author        = {Nils Wandel and Michael Weinmann and Reinhard Klein},
  title         = {Learning Incompressible Fluid Dynamics from Scratch -- Towards Fast, Differentiable Fluid Models that Generalize},
  year          = {2020},
  month         = jun,
  abstract      = {Fast and stable fluid simulations are an essential prerequisite for applications ranging from computer-generated imagery to computer-aided design in research and development. However, solving the partial differential equations of incompressible fluids is a challenging task and traditional numerical approximation schemes come at high computational costs. Recent deep learning based approaches promise vast speed-ups but do not generalize to new fluid domains, require fluid simulation data for training, or rely on complex pipelines that outsource major parts of the fluid simulation to traditional methods. In this work, we propose a novel physics-constrained training approach that generalizes to new fluid domains, requires no fluid simulation data, and allows convolutional neural networks to map a fluid state from time-point t to a subsequent state at time t + dt in a single forward pass. This simplifies the pipeline to train and evaluate neural fluid models. After training, the framework yields models that are capable of fast fluid simulations and can handle various fluid phenomena including the Magnus effect and Karman vortex streets. We present an interactive real-time demo to show the speed and generalization capabilities of our trained models. Moreover, the trained neural networks are efficient differentiable fluid solvers as they offer a differentiable update step to advance the fluid simulation in time. We exploit this fact in a proof-of-concept optimal control experiment. Our models significantly outperform a recent differentiable fluid solver in terms of computational speed and accuracy.},
  archiveprefix = {arXiv},
  comment       = {Unsupervised learning scheme, maybe CNN, to solve Navier Stokes equations, simulating incompressible fluid motion.},
  eprint        = {2006.08762},
  file          = {:http\://arxiv.org/pdf/2006.08762v3:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Cirstea2018,
  author        = {Razvan-Gabriel Cirstea and Darius-Valer Micu and Gabriel-Marcel Muresan and Chenjuan Guo and Bin Yang},
  title         = {Correlated Time Series Forecasting using Deep Neural Networks: A Summary of Results},
  year          = {2018},
  month         = aug,
  abstract      = {Cyber-physical systems often consist of entities that interact with each other over time. Meanwhile, as part of the continued digitization of industrial processes, various sensor technologies are deployed that enable us to record time-varying attributes (a.k.a., time series) of such entities, thus producing correlated time series. To enable accurate forecasting on such correlated time series, this paper proposes two models that combine convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The first model employs a CNN on each individual time series, combines the convoluted features, and then applies an RNN on top of the convoluted features in the end to enable forecasting. The second model adds additional auto-encoders into the individual CNNs, making the second model a multi-task learning model, which provides accurate and robust forecasting. Experiments on two real-world correlated time series data set suggest that the proposed two models are effective and outperform baselines in most settings. This report extends the paper "Correlated Time Series Forecasting using Multi-Task Deep Neural Networks," to appear in ACM CIKM 2018, by providing additional experimental results.},
  archiveprefix = {arXiv},
  comment       = {Applying deep learning to highly spatially -and temporally correlated data, such as meterological data. Though the paper itself is general, i.e. not applied to met-data.},
  eprint        = {1808.09794},
  file          = {:http\://arxiv.org/pdf/1808.09794v2:PDF;:Cirstea2018.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Book{Maraun_2017,
  author    = {Douglas Maraun and Martin Widmann},
  publisher = {Cambridge University Press},
  title     = {Statistical Downscaling and Bias Correction for Climate Research},
  year      = {2017},
  address   = {Cambridge},
  isbn      = {9781107588783},
  month     = {dec},
  doi       = {10.1017/9781107588783},
  file      = {:Maraun2017.pdf:PDF},
}

@Misc{MOI2021,
  author    = {{Mercator Ocean International}},
  title     = {Arctic Ocean - High resolution Sea Ice Concentration and Sea Ice Type},
  year      = {2021},
  doi       = {10.48670/MOI-00122},
  file      = {:Dinessen2021.pdf:PDF},
  keywords  = {oceanography},
  language  = {en},
  publisher = {Mercator Ocean International},
}

@Misc{JETSI2014,
  author    = {{JCOMM Expert Team on Sea Ice}},
  title     = {Sea-Ice Nomenclature: snapshot of the WMO Sea Ice Nomenclature WMO No. 259, volume 1 – Terminology and Codes; Volume II – Illustrated Glossary and III – International System of Sea-Ice Symbols) .},
  year      = {2014},
  doi       = {10.25607/OBP-1515},
  file      = {:Sea_Ice_Nomenclature_March_2014.pdf:PDF},
  publisher = {WMO-JCOMM},
}

@Article{Melsom2019,
  author    = {Arne Melsom and Cyril Palerme and Malte Müller},
  journal   = {Ocean Science},
  title     = {Validation metrics for ice edge position forecasts},
  year      = {2019},
  month     = {may},
  number    = {3},
  pages     = {615--630},
  volume    = {15},
  comment   = {paper presenting Batch Norm technique for standardized gradient flow during backpropagation},
  doi       = {10.5194/os-15-615-2019},
  file      = {Validation metrics for ice edge position forecasts:Melsom2019.pdf:PDF},
  publisher = {Copernicus {GmbH}},
}

@Article{Goessling2016,
  author    = {H. F. Goessling and S. Tietsche and J. J. Day and E. Hawkins and T. Jung},
  journal   = {Geophysical Research Letters},
  title     = {Predictability of the Arctic sea ice edge},
  year      = {2016},
  month     = {feb},
  number    = {4},
  pages     = {1642--1650},
  volume    = {43},
  doi       = {10.1002/2015gl067232},
  file      = {Article that introduces IIEE:Goessling2016.pdf:PDF},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Goessling2018,
  author    = {H. F. Goessling and T. Jung},
  journal   = {Quarterly Journal of the Royal Meteorological Society},
  title     = {A probabilistic verification score for contours: Methodology and application to Arctic ice-edge forecasts},
  year      = {2018},
  month     = apr,
  number    = {712},
  pages     = {735--743},
  volume    = {144},
  doi       = {10.1002/qj.3242},
  file      = {Article that introduces SPS:Goessling2018.pdf:PDF},
  publisher = {Wiley},
}

@Article{Micikevicius2017,
  author        = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
  title         = {Mixed Precision Training},
  year          = {2017},
  month         = oct,
  abstract      = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
  archiveprefix = {arXiv},
  comment       = {Paper presenting mixed precision training. Mixed precision was considered for use in the thesis during model training, as it decreases the memory needed for a training sample, speed up computations as well as decreases the memory bandwidth. However, mixed precision proved to be more difficult to implement than the docs suggested. With unclear descriptions and guides pointing towards different direction. Thus mixed precision was not pursued further, as the nvidia A100 GPU with 80GB of memory is able to fit a 1km model without decreasing sample size.},
  eprint        = {1710.03740},
  file          = {:http\://arxiv.org/pdf/1710.03740v3:PDF},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
}

@Misc{TwoSigma,
  author       = {Two Sigma},
  howpublished = {webpage},
  title        = {A Workaround for Non-Determinism in TensorFlow},
  comment      = {Web-article discussing some workarounds for non-deterministic behaviour resulting from training models on a GPU},
}

@Article{Casati2008,
  author    = {B. Casati and L. J. Wilson and D. B. Stephenson and P. Nurmi and A. Ghelli and M. Pocernich and U. Damrath and E. E. Ebert and B. G. Brown and S. Mason},
  journal   = {Meteorological Applications},
  title     = {Forecast verification: current status and future directions},
  year      = {2008},
  number    = {1},
  pages     = {3--18},
  volume    = {15},
  doi       = {10.1002/met.52},
  publisher = {Wiley},
}

@Article{Dukhovskoy2015,
  author    = {Dmitry S. Dukhovskoy and Jonathan Ubnoske and Edward Blanchard-Wrigglesworth and Hannah R. Hiester and Andrey Proshutinsky},
  journal   = {Journal of Geophysical Research: Oceans},
  title     = {Skill metrics for evaluation and comparison of sea ice models},
  year      = {2015},
  month     = {sep},
  number    = {9},
  pages     = {5910--5931},
  volume    = {120},
  comment   = {Paper introducing the Modified Hausdorff Distance as a ice edge metrics},
  doi       = {10.1002/2015jc010989},
  publisher = {American Geophysical Union ({AGU})},
}

@Misc{IMO2017,
  author       = {International Maritime Organization},
  howpublished = {webpage},
  month        = jan,
  title        = {International Code for Ships Operating in Polar Waters (Polar Code)},
  year         = {2017},
  abstract     = {IMO's International Code for Ships Operating in Polar Waters (Polar Code) is mandatory under both the International Convention for the Safety of Life at Sea (SOLAS) and the International Convention for the Prevention of Pollution from Ships (MARPOL). The Polar Code covers the full range of design, construction, equipment, operational, training, search and rescue and environmental protection matters relevant to ships operating in the inhospitable waters surrounding the two poles. The Polar Code entered into force on 1 January 2017.

The Polar Code and SOLAS amendments were adopted during the 94th session of IMO’s Maritime Safety Committee (MSC), in November 2014; the environmental provisions and MARPOL amendments were adopted during the 68th session of the Marine Environment Protection Committee (MEPC) in May 2015.},
  comment      = {Webpage announcing the Polar Code, which covers the full range of shipping-related matters relevant to navigation in Arctic waters. Not really a soruce, (since the Polar Code is not freely available...), but one standard set, which was mentioned by Cyril, is that ships may be qualified to operate in waters containing a SIC up to 10%.},
  url          = {https://www.imo.org/en/OurWork/Safety/Pages/polar-code.aspx},
}

@Article{Ho2010,
  author    = {Joshua Ho},
  journal   = {Marine Policy},
  title     = {The implications of Arctic sea ice decline on shipping},
  year      = {2010},
  month     = {may},
  number    = {3},
  pages     = {713--715},
  volume    = {34},
  doi       = {10.1016/j.marpol.2009.10.009},
  publisher = {Elsevier {BV}},
}

@Online{reback2020pandas,
  author    = {The pandas development team},
  doi       = {10.5281/zenodo.3509134},
  month     = feb,
  publisher = {Zenodo},
  title     = {pandas-dev/pandas: Pandas},
  url       = {https://doi.org/10.5281/zenodo.3509134},
  version   = {latest},
  year      = {2020},
}

@InProceedings{mckinney-proc-scipy-2010,
  author    = {{W}es {M}c{K}inney},
  booktitle = {{P}roceedings of the 9th {P}ython in {S}cience {C}onference},
  title     = {{D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython},
  year      = {2010},
  editor    = {{S}t\'efan van der {W}alt and {J}arrod {M}illman},
  pages     = {56 - 61},
  doi       = {10.25080/Majora-92bf1922-00a},
}

@Article{Shelhamer2017,
  author    = {Evan Shelhamer and Jonathan Long and Trevor Darrell},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Fully Convolutional Networks for Semantic Segmentation},
  year      = {2017},
  month     = {apr},
  number    = {4},
  pages     = {640--651},
  volume    = {39},
  comment   = {Paper presenting the FCN model for semantic image segmentation},
  doi       = {10.1109/tpami.2016.2572683},
  file      = {:Long2017.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Mueller2017,
  author    = {Malte Müller and Yurii Batrak and J{\o}rn Kristiansen and Morten A. {\O}. K{\o}ltzow and Gunnar Noer and Anton Korosov},
  journal   = {Monthly Weather Review},
  title     = {Characteristics of a Convective-Scale Weather Forecasting System for the European Arctic},
  year      = {2017},
  month     = {dec},
  number    = {12},
  pages     = {4771--4787},
  volume    = {145},
  comment   = {Paper presenting the convective-scale atmospheric prediction system AROME ARCTIC

Section 3.
The Cold Air Outbreak (CAO) phenomena is described. A CAO is an event that occurs during winter, where cold air masses are transported from ice-covered towards lower latitude ice-free areas. As I have understood from discussions with Malte and Cyril, a CAO would cause a movement surge for the Sea Ice to translate towards lower latitudes. Could potentially also cause leads to form in the higher latitude sea ice where the cold air masses are originated and transported away from. However, might be difficult to detect such an event for short timescales, especially with regards to the limited observations available in the arctic. Thus, as Malte suggested, an interesting experiment would be to simulate a CAO using artifically constructed AROME Arctic data, and inspect how the model responds.},
  doi       = {10.1175/mwr-d-17-0194.1},
  file      = {:Muller2017.pdf:PDF},
  publisher = {American Meteorological Society},
}

@Article{Lin2017,
  author        = {Tsung-Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollár},
  title         = {Focal Loss for Dense Object Detection},
  year          = {2017},
  month         = aug,
  abstract      = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  archiveprefix = {arXiv},
  eprint        = {1708.02002},
  file          = {:http\://arxiv.org/pdf/1708.02002v2:PDF;:Lin2018.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Misc{He2015,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1502.01852},
  file      = {:He2015.pdf:PDF},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  publisher = {arXiv},
}

@InProceedings{Krizhevsky2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  year      = {2012},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {25},
  comment   = {Paper presenting the AlexNet CNN},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
}

@Article{Wang2017,
  author    = {Lei Wang and K. Scott and David Clausi},
  journal   = {Remote Sensing},
  title     = {Sea Ice Concentration Estimation during Freeze-Up from {SAR} Imagery Using a Convolutional Neural Network},
  year      = {2017},
  month     = {apr},
  number    = {5},
  pages     = {408},
  volume    = {9},
  comment   = {Paper presenting a LandSea mask interpolation technique for should be but can't NaN values covered by a mask.},
  doi       = {10.3390/rs9050408},
  file      = {:Wang2017.pdf:PDF},
  publisher = {{MDPI} {AG}},
}

@Article{He2015a,
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title         = {Deep Residual Learning for Image Recognition},
  year          = {2015},
  month         = dec,
  abstract      = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  comment       = {Paper presenting the ResNet architecture},
  eprint        = {1512.03385},
  file          = {:http\://arxiv.org/pdf/1512.03385v1:PDF;:He2015(resnet).pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Batrak2018,
  author    = {Yurii Batrak and Ekaterina Kourzeneva and Mariken Homleid},
  journal   = {Geoscientific Model Development},
  title     = {Implementation of a simple thermodynamic sea ice scheme, {SICE} version 1.0-38h1, within the {ALADIN}{\textendash}{HIRLAM} numerical weather prediction system version 38h1},
  year      = {2018},
  month     = {aug},
  number    = {8},
  pages     = {3347--3368},
  volume    = {11},
  comment   = {Article presenting the snow-on-ice variable present in AROME Arctic from (mid-2018?). The introduction of this variable shifted the T2M bias over sea ice, effectively limiting the dataset which I can use to train my unet,},
  doi       = {10.5194/gmd-11-3347-2018},
  file      = {:Batrak2018.pdf:PDF},
  publisher = {Copernicus {GmbH}},
}

@Article{Spreen2011,
  author    = {Gunnar Spreen and Ron Kwok and Dimitris Menemenlis},
  journal   = {Geophysical Research Letters},
  title     = {Trends in Arctic sea ice drift and role of wind forcing: 1992-2009},
  year      = {2011},
  month     = {oct},
  number    = {19},
  pages     = {n/a--n/a},
  volume    = {38},
  comment   = {Surface winds influence Sea Ice Drift},
  doi       = {10.1029/2011gl048970},
  file      = {:Spreen2008.pdf:PDF},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Yu2020,
  author    = {Xiaoyong Yu and Annette Rinke and Wolfgang Dorn and Gunnar Spreen and Christof Lüpkes and Hiroshi Sumata and Vladimir M. Gryanik},
  journal   = {The Cryosphere},
  title     = {Evaluation of Arctic sea ice drift and its dependency on near-surface wind and sea ice conditions in the coupled regional climate model {HIRHAM}{\textendash}{NAOSIM}},
  year      = {2020},
  month     = {may},
  number    = {5},
  pages     = {1727--1746},
  volume    = {14},
  comment   = {Sea Ice Drift Speed inverse proportional to SIC},
  doi       = {10.5194/tc-14-1727-2020},
  publisher = {Copernicus {GmbH}},
}

@Article{Ioffe2015,
  author        = {Sergey Ioffe and Christian Szegedy},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year          = {2015},
  month         = feb,
  abstract      = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  eprint        = {1502.03167},
  file          = {:http\://arxiv.org/pdf/1502.03167v3:PDF;:Ioffe2015.pdf:PDF},
  keywords      = {cs.LG},
  primaryclass  = {cs.LG},
}

@Article{Wu2018,
  author        = {Yuxin Wu and Kaiming He},
  title         = {Group Normalization},
  year          = {2018},
  month         = mar,
  abstract      = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  archiveprefix = {arXiv},
  comment       = {Paper presenting group normalization for standardized gradient flow given small batch sizes},
  eprint        = {1803.08494},
  file          = {:http\://arxiv.org/pdf/1803.08494v3:PDF;:Wu2018.pdf:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Ludwig2020,
  author    = {Valentin Ludwig and Gunnar Spreen and Leif Toudal Pedersen},
  journal   = {Remote Sensing},
  title     = {Evaluation of a New Merged Sea-Ice Concentration Dataset at 1 km Resolution from Thermal Infrared and Passive Microwave Satellite Data in the Arctic},
  year      = {2020},
  month     = {sep},
  number    = {19},
  pages     = {3183},
  volume    = {12},
  comment   = {Paper presenting a merged SIC rs product on a 1km grid},
  doi       = {10.3390/rs12193183},
  file      = {:Ludwig2020.pdf:PDF},
  publisher = {{MDPI} {AG}},
}

@Misc{tensorflow2015-whitepaper,
  author = {Mart\'{i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  note   = {Software available from tensorflow.org},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  url    = {https://www.tensorflow.org/},
}

@Article{Cavalieri2012,
  author    = {D. J. Cavalieri and C. L. Parkinson},
  journal   = {The Cryosphere},
  title     = {Arctic sea ice variability and trends, 1979{\textendash}2010},
  year      = {2012},
  month     = {aug},
  number    = {4},
  pages     = {881--889},
  volume    = {6},
  doi       = {10.5194/tc-6-881-2012},
  publisher = {Copernicus {GmbH}},
}

@Article{Comiso2017,
  author    = {Josefino C. Comiso and Walter N. Meier and Robert Gersten},
  journal   = {Journal of Geophysical Research: Oceans},
  title     = {Variability and trends in the Arctic Sea ice cover: Results from different techniques},
  year      = {2017},
  month     = {aug},
  number    = {8},
  pages     = {6883--6900},
  volume    = {122},
  doi       = {10.1002/2017jc012768},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Notz2020,
  author    = {Dirk Notz and SIMIP Community},
  journal   = {Geophysical Research Letters},
  title     = {Arctic Sea Ice in {CMIP}6},
  year      = {2020},
  month     = {may},
  number    = {10},
  volume    = {47},
  doi       = {10.1029/2019gl086749},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Eguiluz2016,
  author    = {Victor M. Egu{\'{\i}}luz and Juan Fern{\'{a}}ndez-Gracia and Xabier Irigoien and Carlos M. Duarte},
  journal   = {Scientific Reports},
  title     = {A quantitative assessment of Arctic shipping in 2010{\textendash}2014},
  year      = {2016},
  month     = {aug},
  number    = {1},
  volume    = {6},
  doi       = {10.1038/srep30682},
  publisher = {Springer Science and Business Media {LLC}},
}

@Misc{nextsimdata2020,
  author    = {{European Union-Copernicus Marine Service}},
  title     = {Arctic Ocean Sea Ice Analysis and Forecast},
  year      = {2020},
  doi       = {10.48670/MOI-00004},
  keywords  = {oceanography},
  language  = {en},
  publisher = {Mercator Ocean International},
}

@Article{Williams2021,
  author    = {Timothy Williams and Anton Korosov and Pierre Rampal and Einar {\'{O}}lason},
  journal   = {The Cryosphere},
  title     = {Presentation and evaluation of the Arctic sea ice forecasting system {neXtSIM}-F},
  year      = {2021},
  month     = {jul},
  number    = {7},
  pages     = {3207--3227},
  volume    = {15},
  comment   = {Paper presenting the neXtSIM physical sea ice model},
  doi       = {10.5194/tc-15-3207-2021},
  file      = {:Williams2021.pdf:PDF},
  publisher = {Copernicus {GmbH}},
}

@Article{Liu2021,
  author    = {Yang Liu and Laurens Bogaardt and Jisk Attema and Wilco Hazeleger},
  journal   = {Monthly Weather Review},
  title     = {Extended Range Arctic Sea Ice Forecast with Convolutional Long-Short Term Memory Networks},
  year      = {2021},
  month     = {mar},
  doi       = {10.1175/mwr-d-20-0113.1},
  file      = {:Liu2021.pdf:PDF},
  publisher = {American Meteorological Society},
}

@Article{Grigoryev2022,
  author         = {Grigoryev, Timofey and Verezemskaya, Polina and Krinitskiy, Mikhail and Anikin, Nikita and Gavrikov, Alexander and Trofimov, Ilya and Balabin, Nikita and Shpilman, Aleksei and Eremchenko, Andrei and Gulev, Sergey and Burnaev, Evgeny and Vanovskiy, Vladimir},
  journal        = {Remote Sensing},
  title          = {Data-Driven Short-Term Daily Operational Sea Ice Regional Forecasting},
  year           = {2022},
  issn           = {2072-4292},
  number         = {22},
  volume         = {14},
  abstract       = {Global warming has made the Arctic increasingly available for marine operations and created a demand for reliable operational sea ice forecasts to increase safety. Because ocean-ice numerical models are highly computationally intensive, relatively lightweight ML-based methods may be more efficient for sea ice forecasting. Many studies have exploited different deep learning models alongside classical approaches for predicting sea ice concentration in the Arctic. However, only a few focus on daily operational forecasts and consider the real-time availability of data needed for marine operations. In this article, we aim to close this gap and investigate the performance of the U-Net model trained in two regimes for predicting sea ice for up to the next 10 days. We show that this deep learning model can outperform simple baselines by a significant margin, and we can improve the model&rsquo;s quality by using additional weather data and training on multiple regions to ensure its generalization abilities. As a practical outcome, we build a fast and flexible tool that produces operational sea ice forecasts in the Barents Sea, the Labrador Sea, and the Laptev Sea regions.},
  article-number = {5837},
  doi            = {10.3390/rs14225837},
  file           = {:Grigoryev2022.pdf:PDF},
  url            = {https://www.mdpi.com/2072-4292/14/22/5837},
}

@Article{Serreze2019,
  author   = {Serreze, Mark C. and Meier, Walter N.},
  journal  = {Annals of the New York Academy of Sciences},
  title    = {The Arctic's sea ice cover: trends, variability, predictability, and comparisons to the Antarctic},
  year     = {2019},
  number   = {1},
  pages    = {36-53},
  volume   = {1436},
  abstract = {Abstract As assessed over the period of satellite observations, October 1978 to present, there are downward linear trends in Arctic sea ice extent for all months, largest at the end of the melt season in September. The ice cover is also thinning. Downward trends in extent and thickness have been accompanied by pronounced interannual and multiyear variability, forced by both the atmosphere and ocean. As the ice thins, its response to atmospheric and oceanic forcing may be changing. In support of a busier Arctic, there is a growing need to predict ice conditions on a variety of time and space scales. A major challenge to providing seasonal scale predictions is the 7–10 days limit of numerical weather prediction. While a seasonally ice-free Arctic Ocean is likely well within this century, there is much uncertainty in the timing. This reflects differences in climate model structure, the unknown evolution of anthropogenic forcing, and natural climate variability. In sharp contrast to the Arctic, Antarctic sea ice extent, while highly variable, has increased slightly over the period of satellite observations. The reasons for this different behavior remain to be resolved, but responses to changing atmospheric circulation patterns appear to play a strong role.},
  doi      = {https://doi.org/10.1111/nyas.13856},
  file     = {:Serreze2018.pdf:PDF},
  keywords = {Arctic, Antarctic, sea ice, trends, variability, predictability},
}

@Article{Johnson2019,
  author  = {Johnson, S. J. and Stockdale, T. N. and Ferranti, L. and Balmaseda, M. A. and Molteni, F. and Magnusson, L. and Tietsche, S. and Decremer, D. and Weisheimer, A. and Balsamo, G. and Keeley, S. P. E. and Mogensen, K. and Zuo, H. and Monge-Sanz, B. M.},
  journal = {Geoscientific Model Development},
  title   = {SEAS5: the new ECMWF seasonal forecast system},
  year    = {2019},
  number  = {3},
  pages   = {1087--1117},
  volume  = {12},
  doi     = {10.5194/gmd-12-1087-2019},
  url     = {https://gmd.copernicus.org/articles/12/1087/2019/},
}

@Article{Lavergne2019,
  author  = {Lavergne, T. and S{\o}rensen, A. M. and Kern, S. and Tonboe, R. and Notz, D. and Aaboe, S. and Bell, L. and Dybkj{\ae}r, G. and Eastwood, S. and Gabarro, C. and Heygster, G. and Killie, M. A. and Brandt Kreiner, M. and Lavelle, J. and Saldo, R. and Sandven, S. and Pedersen, L. T.},
  journal = {The Cryosphere},
  title   = {Version 2 of the EUMETSAT OSI SAF and ESA CCI sea-ice concentration climate data records},
  year    = {2019},
  number  = {1},
  pages   = {49--78},
  volume  = {13},
  doi     = {10.5194/tc-13-49-2019},
  url     = {https://tc.copernicus.org/articles/13/49/2019/},
}

@Article{Hersbach2020,
  author   = {Hersbach, Hans and Bell, Bill and Berrisford, Paul and Hirahara, Shoji and Horányi, András and Muñoz-Sabater, Joaquín and Nicolas, Julien and Peubey, Carole and Radu, Raluca and Schepers, Dinand and Simmons, Adrian and Soci, Cornel and Abdalla, Saleh and Abellan, Xavier and Balsamo, Gianpaolo and Bechtold, Peter and Biavati, Gionata and Bidlot, Jean and Bonavita, Massimo and De Chiara, Giovanna and Dahlgren, Per and Dee, Dick and Diamantakis, Michail and Dragani, Rossana and Flemming, Johannes and Forbes, Richard and Fuentes, Manuel and Geer, Alan and Haimberger, Leo and Healy, Sean and Hogan, Robin J. and Hólm, Elías and Janisková, Marta and Keeley, Sarah and Laloyaux, Patrick and Lopez, Philippe and Lupu, Cristina and Radnoti, Gabor and de Rosnay, Patricia and Rozum, Iryna and Vamborg, Freja and Villaume, Sebastien and Thépaut, Jean-Noël},
  journal  = {Quarterly Journal of the Royal Meteorological Society},
  title    = {The ERA5 global reanalysis},
  year     = {2020},
  number   = {730},
  pages    = {1999-2049},
  volume   = {146},
  abstract = {Abstract Within the Copernicus Climate Change Service (C3S), ECMWF is producing the ERA5 reanalysis which, once completed, will embody a detailed record of the global atmosphere, land surface and ocean waves from 1950 onwards. This new reanalysis replaces the ERA-Interim reanalysis (spanning 1979 onwards) which was started in 2006. ERA5 is based on the Integrated Forecasting System (IFS) Cy41r2 which was operational in 2016. ERA5 thus benefits from a decade of developments in model physics, core dynamics and data assimilation. In addition to a significantly enhanced horizontal resolution of 31 km, compared to 80 km for ERA-Interim, ERA5 has hourly output throughout, and an uncertainty estimate from an ensemble (3-hourly at half the horizontal resolution). This paper describes the general set-up of ERA5, as well as a basic evaluation of characteristics and performance, with a focus on the dataset from 1979 onwards which is currently publicly available. Re-forecasts from ERA5 analyses show a gain of up to one day in skill with respect to ERA-Interim. Comparison with radiosonde and PILOT data prior to assimilation shows an improved fit for temperature, wind and humidity in the troposphere, but not the stratosphere. A comparison with independent buoy data shows a much improved fit for ocean wave height. The uncertainty estimate reflects the evolution of the observing systems used in ERA5. The enhanced temporal and spatial resolution allows for a detailed evolution of weather systems. For precipitation, global-mean correlation with monthly-mean GPCP data is increased from 67\% to 77\%. In general, low-frequency variability is found to be well represented and from 10 hPa downwards general patterns of anomalies in temperature match those from the ERA-Interim, MERRA-2 and JRA-55 reanalyses.},
  doi      = {https://doi.org/10.1002/qj.3803},
  eprint   = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3803},
  keywords = {climate reanalysis, Copernicus Climate Change Service, data assimilation, ERA5, historical observations},
  url      = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3803},
}

@Article{Dee2011,
  author   = {Dee, D. P. and Uppala, S. M. and Simmons, A. J. and Berrisford, P. and Poli, P. and Kobayashi, S. and Andrae, U. and Balmaseda, M. A. and Balsamo, G. and Bauer, P. and Bechtold, P. and Beljaars, A. C. M. and van de Berg, L. and Bidlot, J. and Bormann, N. and Delsol, C. and Dragani, R. and Fuentes, M. and Geer, A. J. and Haimberger, L. and Healy, S. B. and Hersbach, H. and Hólm, E. V. and Isaksen, L. and Kållberg, P. and Köhler, M. and Matricardi, M. and McNally, A. P. and Monge-Sanz, B. M. and Morcrette, J.-J. and Park, B.-K. and Peubey, C. and de Rosnay, P. and Tavolato, C. and Thépaut, J.-N. and Vitart, F.},
  journal  = {Quarterly Journal of the Royal Meteorological Society},
  title    = {The ERA-Interim reanalysis: configuration and performance of the data assimilation system},
  year     = {2011},
  number   = {656},
  pages    = {553-597},
  volume   = {137},
  abstract = {Abstract ERA-Interim is the latest global atmospheric reanalysis produced by the European Centre for Medium-Range Weather Forecasts (ECMWF). The ERA-Interim project was conducted in part to prepare for a new atmospheric reanalysis to replace ERA-40, which will extend back to the early part of the twentieth century. This article describes the forecast model, data assimilation method, and input datasets used to produce ERA-Interim, and discusses the performance of the system. Special emphasis is placed on various difficulties encountered in the production of ERA-40, including the representation of the hydrological cycle, the quality of the stratospheric circulation, and the consistency in time of the reanalysed fields. We provide evidence for substantial improvements in each of these aspects. We also identify areas where further work is needed and describe opportunities and objectives for future reanalysis projects at ECMWF. Copyright © 2011 Royal Meteorological Society},
  doi      = {https://doi.org/10.1002/qj.828},
  eprint   = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.828},
  keywords = {ERA-40, 4D-Var, hydrological cycle, stratospheric circulation, observations, forecast model},
  url      = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.828},
}

@Article{Balmaseda2013,
  author   = {Balmaseda, Magdalena Alonso and Mogensen, Kristian and Weaver, Anthony T.},
  journal  = {Quarterly Journal of the Royal Meteorological Society},
  title    = {Evaluation of the ECMWF ocean reanalysis system ORAS4},
  year     = {2013},
  number   = {674},
  pages    = {1132-1161},
  volume   = {139},
  abstract = {Abstract A new operational ocean reanalysis system (ORAS4) has been implemented at ECMWF. It spans the period 1958 to the present. This article describes its main components and evaluates its quality. The adequacy of ORAS4 for the initialization of seasonal forecasts is discussed, along with the robustness of some prominent climate signals. ORAS4 has been evaluated using different metrics, including comparison with observed ocean currents, RAPID-derived transports, sea-level gauges, and GRACE-derived bottom pressure. Compared to a control ocean model simulation, ORAS4 improves the fit to observations, the interannual variability, and seasonal forecast skill. Some problems have been identified, such as the underestimation of meridional overturning at 26°N, the magnitude of which is shown to be sensitive to the treatment of the coastal observations. ORAS4 shows a clear and robust shallowing trend of the Pacific Equatorial thermocline. It also shows a clear and robust nonlinear trend in the 0–700 m ocean heat content, consistent with other observational estimates. Some aspects of these climate signals are sensitive to the choice of sea-surface temperature product and the specification of the observation-error variances. The global sea-level trend is consistent with the altimeter estimate, but the partition into volume and mass variations is more debatable, as inferred by discrepancies in the trend between ORAS4- and GRACE-derived bottom pressure.},
  doi      = {https://doi.org/10.1002/qj.2063},
  eprint   = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.2063},
  keywords = {ocean reanalysis, climate variability, validation, quality metric, initialization},
  url      = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.2063},
}

@Article{Chin2017,
  author    = {Toshio Michael Chin and Jorge Vazquez-Cuervo and Edward M. Armstrong},
  journal   = {Remote Sensing of Environment},
  title     = {A multi-scale high-resolution analysis of global sea surface temperature},
  year      = {2017},
  month     = {oct},
  pages     = {154--169},
  volume    = {200},
  doi       = {10.1016/j.rse.2017.07.029},
  publisher = {Elsevier {BV}},
}

@Misc{Kristensen2017,
  author    = {Kristensen, Nils M and {JensBDebernard} and {SebastianMaartensson} and {Keguang Wang} and Hedstrom, Kate},
  title     = {Metno/Metroms: Version 0.3 - Before Merge},
  year      = {2017},
  copyright = {Open Access},
  doi       = {10.5281/ZENODO.1046114},
  publisher = {Zenodo},
}

@InProceedings{NVIDIA,
  author = {NVIDIA},
  title  = {NVIDIA A100 Tensor Core GPU Architecture UNPRECEDENTED ACCELERATION AT EVERY SCALE},
  file   = {:/home/arefk/uio/MScThesis_AreKvanum2022_SeaIceML/thesis/bib/nvidia-ampere-architecture-whitepaper.pdf:PDF},
}

@Article{Roehrs2022,
  author   = {Johannes Röhrs and Yvonne Gusdal and Edel Rikardsen and Marina Duran Moro and Jostein Brændshøi and Nils Melsom Kristensen and Sindre Fritzner and Keguang Wang and Ann Kristin Sperrevik and Martina Idžanović and Thomas Lavergne and Jens Debernard and Kai H. Christensen},
  title    = {"in prep for GMD" An operational data-assimilative coupled ocean and sea ice ensembleprediction model for the Barents Sea and Svalbard},
  year     = {2022},
  pages    = {20},
  abstract = {An operational ocean and sea ice forecast model, Barents-2.5 , is implemented at MET Norway for short-term forecasting of the ocean’s state at the coast off Northern Norway, the Barents Sea, and waters around Svalbard. Primary forecast parameters are the extent of the ice cover, sea surface temperature (SST), and ocean currents. The model is also an
substantial input for drift modeling of pollutants, ice berg, and in search-and-rescue pertinent applications in the Arctic domain. Barents-2.5 has recently been upgraded to include an Ensemble Prediction System with 24 daily realizations of the model state. Sea ice cover, SST and in-situ hydrography are constrained through a Ensemble-Kalman filter (EnKF) data assimilation scheme executed in daily forecast cycles with lead time up to 66 hours. While the ocean circulation is not directly constrained
by assimilation of ocean currents, the model ensemble represents the given uncertainty in the short-term current field by retaining the current state for each member throughout forecast cycles. We present here a model validation in terms of sea ice concentration, SST and in-situ hydrography, the performance of the ensemble to represent the models uncertainty, and the performance of the EnKF to constrain the model state. Finally, a discussion of forecast skill for selected variables is provided.},
  comment  = {"GMD" = Geoscientific Model Development?},
  file     = {:Roehrs2022.pdf:PDF},
}

@TechReport{Hunke2015,
  author      = {Elizabeth C. Hunke and William H. Lipscomb and Adrian K. Turner and Nicole Jeffery and Scott Elliott},
  institution = {Los Alamos National Laboratory},
  title       = {CICE: the Los Alamos Sea Ice Model Documentation and Software User’s Manual Version 5.1 LA-CC-06-012},
  year        = {2015},
  address     = {Los Alamos NM 87545},
  month       = jul,
  type        = {techreport},
}

@Article{Olason2022,
  author    = {Einar {\'{O}}lason and Guillaume Boutin and Anton Korosov and Pierre Rampal and Timothy Williams and Madlen Kimmritz and V{\'{e}}ronique Dansereau and Abdoulaye Samak{\'{e}}},
  journal   = {Journal of Advances in Modeling Earth Systems},
  title     = {A New Brittle Rheology and Numerical Framework for Large-Scale Sea-Ice Models},
  year      = {2022},
  month     = {aug},
  number    = {8},
  volume    = {14},
  doi       = {10.1029/2021ms002685},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Dansereau2016,
  author    = {V{\'{e}}ronique Dansereau and J{\'{e}}r{\^{o}}me Weiss and Pierre Saramito and Philippe Lattes},
  journal   = {The Cryosphere},
  title     = {A Maxwell elasto-brittle rheology for sea ice modelling},
  year      = {2016},
  month     = {jul},
  number    = {3},
  pages     = {1339--1359},
  volume    = {10},
  doi       = {10.5194/tc-10-1339-2016},
  publisher = {Copernicus {GmbH}},
}

@Article{Hibler1979,
  author    = {W. D. Hibler},
  journal   = {Journal of Physical Oceanography},
  title     = {A Dynamic Thermodynamic Sea Ice Model},
  year      = {1979},
  month     = {jul},
  number    = {4},
  pages     = {815--846},
  volume    = {9},
  doi       = {10.1175/1520-0485(1979)009<0815:adtsim>2.0.co;2},
  publisher = {American Meteorological Society},
}

@Article{Hunke1997,
  author    = {E. C. Hunke and J. K. Dukowicz},
  journal   = {Journal of Physical Oceanography},
  title     = {An Elastic{\textendash}Viscous{\textendash}Plastic Model for Sea Ice Dynamics},
  year      = {1997},
  month     = {sep},
  number    = {9},
  pages     = {1849--1867},
  volume    = {27},
  doi       = {10.1175/1520-0485(1997)027<1849:aevpmf>2.0.co;2},
  publisher = {American Meteorological Society},
}

@Article{Sakov2008,
  author    = {Pavel Sakov and Peter R. Oke},
  journal   = {Tellus A: Dynamic Meteorology and Oceanography},
  title     = {A deterministic formulation of the ensemble Kalman filter: an alternative to ensemble square root filters},
  year      = {2008},
  month     = {jan},
  number    = {2},
  pages     = {361},
  volume    = {60},
  doi       = {10.1111/j.1600-0870.2007.00299.x},
  publisher = {Stockholm University Press},
}

@Article{Kern2019,
  author    = {Stefan Kern and Thomas Lavergne and Dirk Notz and Leif Toudal Pedersen and Rasmus Tage Tonboe and Roberto Saldo and Atle MacDonald S{\o}rensen},
  journal   = {The Cryosphere},
  title     = {Satellite passive microwave sea-ice concentration data set intercomparison: closed ice and ship-based observations},
  year      = {2019},
  month     = {dec},
  number    = {12},
  pages     = {3261--3307},
  volume    = {13},
  doi       = {10.5194/tc-13-3261-2019},
  file      = {:Kern2019.pdf:PDF},
  publisher = {Copernicus {GmbH}},
}

@Article{Wagner2020,
  author    = {Penelope Mae Wagner and Nick Hughes and Pascale Bourbonnais and Julienne Stroeve and Lasse Rabenstein and Uma Bhatt and Joe Little and Helen Wiggins and Andrew Fleming},
  journal   = {Polar Geography},
  title     = {Sea-ice information and forecast needs for industry maritime stakeholders},
  year      = {2020},
  month     = {jun},
  number    = {2-3},
  pages     = {160--187},
  volume    = {43},
  doi       = {10.1080/1088937x.2020.1766592},
  file      = {:Wagner2020.pdf:PDF},
  publisher = {Informa {UK} Limited},
}

@Article{Palerme2019,
  author    = {Cyril Palerme and Malte Müller and Arne Melsom},
  journal   = {Geophysical Research Letters},
  title     = {An Intercomparison of Verification Scores for Evaluating the Sea Ice Edge Position in Seasonal Forecasts},
  year      = {2019},
  month     = {may},
  number    = {9},
  pages     = {4757--4763},
  volume    = {46},
  comment   = {Section 4 Paragraph 1"
For example, a high
sensitivity to isolated ice patches can be suitable for evaluating the forecast ability to reproduce coastal sea
ice, while this can be inadequate for comparing the general agreement between the forecast and observed
ice edge positions.
"

This is an interesting remark as to why I should avoid computing the MHD for my dataset, however it could be interesting to inspect how the metric performs for high resolution data, as isolated ice patches could be spatially resolved by the ice charts.},
  doi       = {10.1029/2019gl082482},
  file      = {Sea Ice Edge position verification for seasonal forecasting:Palerme2019.pdf:PDF},
  publisher = {American Geophysical Union ({AGU})},
}

@TechReport{Veland2021,
  author      = {S Veland and P Wagner and D Bailey and A Everett and M Goldstein and R Hermann and T Hjort-Larsen and G Hovelsrud and N Hughes and A Kjøl and X Li and A Lynch and M Müller and J Olsen and C Palerme and J L Pedersen and Ø Rinaldo and S Stephenson and T Storelvmo},
  institution = {Svalbard Strategic Grant, Svalbard Science Forum},
  title       = {Knowledge needs in sea ice forecasting for navigation in Svalbard and the High Arctic},
  year        = {2021},
  number      = {NF-rapport 4/2021},
  file        = {:Veland2021.pdf:PDF},
}

@Article{Spreen2008,
  author    = {G. Spreen and L. Kaleschke and G. Heygster},
  journal   = {Journal of Geophysical Research},
  title     = {Sea ice remote sensing using {AMSR}-E 89-{GHz} channels},
  year      = {2008},
  month     = {jan},
  number    = {C2},
  volume    = {113},
  abstract  = {AMRS-2 AIS retrieval algorithm},
  doi       = {10.1029/2005jc003384},
  publisher = {American Geophysical Union ({AGU})},
}

@TechReport{Melsheimer2019,
  author      = {Christian Melsheimer},
  institution = {Institute of Environmental Physics, University of Bremen},
  title       = {ASI Version 5 Sea Ice Concentration User Guide},
  year        = {2019},
  month       = aug,
}

@TechReport{Tonboe2017,
  author      = {Rasmus Tonboe and John Lavelle and R.-Helge Pfeiffer and Eva Howe},
  institution = {Danish Meteorological Institute},
  title       = {Product User Manual for OSI SAF Global Sea Ice Concentration},
  year        = {2017},
  month       = sep,
  number      = {1.6},
  file        = {:Tonboe2017.pdf:PDF},
}

@TechReport{Dinessen2020,
  author      = {Frode Dinessen and Bruce Hackett and Matilde Brandt Kreiner},
  institution = {Norwegian Meteorological Institute},
  title       = {Product User Manual For Regional High Resolution Sea Ice Charts Svalbard and Greenland Region},
  year        = {2020},
  month       = sep,
  file        = {:Dinessen2020.pdf:PDF},
}

@TechReport{Soerensen2021,
  author      = {Atle M Sørensen and Thomas Lavergne and Steinar Eastwood},
  institution = {Norwegian Meteorological Institute},
  title       = {Global Sea Ice Concentration Climate Data Record Product Uses Manual Product OSI-450 \& OSI-430-b},
  year        = {2021},
  month       = feb,
  number      = {2.1},
  file        = {:Sørensen2021:},
}

@Article{Comiso1997,
  author    = {Josefino C. Comiso and Donald J. Cavalieri and Claire L. Parkinson and Per Gloersen},
  journal   = {Remote Sensing of Environment},
  title     = {Passive microwave algorithms for sea ice concentration: A comparison of two techniques},
  year      = {1997},
  month     = {jun},
  number    = {3},
  pages     = {357--384},
  volume    = {60},
  doi       = {10.1016/s0034-4257(96)00220-9},
  publisher = {Elsevier {BV}},
}

@Article{Smith1996,
  author    = {D. M. Smith},
  journal   = {International Journal of Remote Sensing},
  title     = {Extraction of winter total sea-ice concentration in the Greenland and Barents Seas from {SSM}/I data},
  year      = {1996},
  month     = {sep},
  number    = {13},
  pages     = {2625--2646},
  volume    = {17},
  doi       = {10.1080/01431169608949096},
  publisher = {Informa {UK} Limited},
}

@Article{Holland2016,
  author    = {Paul R. Holland and Noriaki Kimura},
  journal   = {Journal of Climate},
  title     = {Observed Concentration Budgets of Arctic and Antarctic Sea Ice},
  year      = {2016},
  month     = {jun},
  number    = {14},
  pages     = {5241--5249},
  volume    = {29},
  doi       = {10.1175/jcli-d-16-0121.1},
  publisher = {American Meteorological Society},
}

@TechReport{Lavelle2016,
  author      = {John Lavelle and Rasmus Tonboe and Tian Tian and R-Helge Pfeiffer and Eva Howe},
  institution = {Danish Meteorological Institute},
  title       = {Product User Manual for the OSI SAF AMSR-2 Global Sea Ice Concentration Product OSI-408},
  year        = {2016},
  month       = aug,
  number      = {1.1},
}

@TechReport{Lavergne2019a,
  author = {Thomas Lavergne and Rasmus Tonboe and John Lavelle and Steinar Eastwood},
  title  = {Algorithm Theoretical Basis Document for the OSI SAF Global Sea Ice Concentration Climate Data Record OSI-450, OSI-430-b},
  year   = {2019},
  month  = mar,
  number = {1.2},
  type   = {techreport},
  file   = {:Lavergne2019a.pdf:PDF},
}

@Misc{Melsheimer2020,
  author    = {Melsheimer, Christian and Spreen, Gunnar},
  title     = {AMSR-E ASI sea ice concentration data, Arctic, version 5.4 (NetCDF) (June 2002 - September 2011)},
  year      = {2020},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  doi       = {10.1594/PANGAEA.919777},
  keywords  = {AMSR-E, Arctic, satellite, Sea ice, sea ice concentration, File content, File name, File format, File size, Uniform resource locator/link to file, Arctic Amplification (AC3)},
  language  = {en},
  publisher = {PANGAEA},
}

@Article{Haiden2022,
  author    = {Haiden, Thomas and Janousek, Martin and Vitart, Frédéric and Ben-Bouallegue, Zied and Ferranti, Laura and Prates, Fernando and Richardson, David},
  title     = {Evaluation of ECMWF forecasts, including the 2021 upgrade},
  year      = {2022},
  doi       = {10.21957/XQNU5O3P},
  publisher = {ECMWF},
}

@Article{Bauer2013,
  author    = {Bauer, Peter and Beljaars, Anton and Ahlgrimm, Maike and Bechtold, Peter and Bidlot, Jean-Raymond and Bonavita, Massimo and Bozzo, Alessio and Forbes, Richard and Hólm, E. and Leutbecher, Martin and Lopez, Philippe and Magnusson, L. and Prates, F. and Rodwell, Mark and Sandu, Irina and Untch, A. and Vitart, Frédéric},
  title     = {Model Cycle 38r2: Components and Performance},
  year      = {2013},
  doi       = {10.21957/XC1R0LJ6L},
  publisher = {ECMWF},
}

@Article{Koeltzow2022,
  author    = {Morten K{\o}ltzow and Harald Schyberg and Eivind St{\o}ylen and Xiaohua Yang},
  journal   = {Polar Research},
  title     = {Value of the Copernicus Arctic Regional Reanalysis ({CARRA}) in representing near-surface temperature and wind speed in the north-east European Arctic},
  year      = {2022},
  month     = {mar},
  volume    = {41},
  doi       = {10.33265/polar.v41.8002},
  publisher = {Norwegian Polar Institute},
}

@Article{Zampieri2019,
  author    = {Lorenzo Zampieri and Helge F. Goessling and Thomas Jung},
  journal   = {Geophysical Research Letters},
  title     = {Predictability of Antarctic Sea Ice Edge on Subseasonal Time Scales},
  year      = {2019},
  month     = {aug},
  number    = {16},
  pages     = {9719--9727},
  volume    = {46},
  doi       = {10.1029/2019gl084096},
  file      = {:Zampieri2019.pdf:PDF},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Jeuring2019,
  author    = {Jelmer Jeuring and Maaike Knol-Kauffman and Anders Sivle},
  journal   = {Polar Geography},
  title     = {Toward valuable weather and sea-ice services for the marine Arctic: exploring user{\textendash}producer interfaces of the Norwegian Meteorological Institute},
  year      = {2019},
  month     = {oct},
  number    = {2-3},
  pages     = {139--159},
  volume    = {43},
  doi       = {10.1080/1088937x.2019.1679270},
  file      = {:Jeuring2019.pdf:PDF},
  publisher = {Informa {UK} Limited},
}

@Article{Jeuring2019a,
  author  = {Jelmer Jeuring and Maaike Knol-Kauffman},
  journal = {Umeå Universitet},
  title   = {Mapping Weather, Water, Ice and Climate Knowledge & Information Needs for Maritime Activities in the Arctic. Survey Report},
  year    = {2019},
  file    = {:Jeuring2019a.pdf:PDF},
}

@Article{Kern2020,
  author    = {Stefan Kern and Thomas Lavergne and Dirk Notz and Leif Toudal Pedersen and Rasmus Tonboe},
  journal   = {The Cryosphere},
  title     = {Satellite passive microwave sea-ice concentration data set inter-comparison for Arctic summer conditions},
  year      = {2020},
  month     = {jul},
  number    = {7},
  pages     = {2469--2493},
  volume    = {14},
  doi       = {10.5194/tc-14-2469-2020},
  publisher = {Copernicus {GmbH}},
}

@Article{Sakov2012,
  author    = {P. Sakov and F. Counillon and L. Bertino and K. A. Lis{\ae}ter and P. R. Oke and A. Korablev},
  journal   = {Ocean Science},
  title     = {{TOPAZ}4: an ocean-sea ice data assimilation system for the North Atlantic and Arctic},
  year      = {2012},
  month     = {aug},
  number    = {4},
  pages     = {633--656},
  volume    = {8},
  doi       = {10.5194/os-8-633-2012},
  publisher = {Copernicus {GmbH}},
}

@InCollection{Zeiler2014,
  author    = {Matthew D. Zeiler and Rob Fergus},
  booktitle = {Computer Vision {\textendash} {ECCV} 2014},
  publisher = {Springer International Publishing},
  title     = {Visualizing and Understanding Convolutional Networks},
  year      = {2014},
  pages     = {818--833},
  doi       = {10.1007/978-3-319-10590-1_53},
  file      = {:Zeiler2014.pdf:PDF},
}

@Article{LeCun1989,
  author    = {Y. LeCun and B. Boser and J. S. Denker and D. Henderson and R. E. Howard and W. Hubbard and L. D. Jackel},
  journal   = {Neural Computation},
  title     = {Backpropagation Applied to Handwritten Zip Code Recognition},
  year      = {1989},
  month     = {dec},
  number    = {4},
  pages     = {541--551},
  volume    = {1},
  doi       = {10.1162/neco.1989.1.4.541},
  file      = {:Lecun1989.pdf:PDF},
  publisher = {{MIT} Press - Journals},
}

@Article{Rumelhart1986,
  author    = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal   = {Nature},
  title     = {Learning representations by back-propagating errors},
  year      = {1986},
  month     = {oct},
  number    = {6088},
  pages     = {533--536},
  volume    = {323},
  doi       = {10.1038/323533a0},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Hornik1989,
  author    = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  journal   = {Neural Networks},
  title     = {Multilayer feedforward networks are universal approximators},
  year      = {1989},
  month     = {jan},
  number    = {5},
  pages     = {359--366},
  volume    = {2},
  doi       = {10.1016/0893-6080(89)90020-8},
  publisher = {Elsevier {BV}},
}

@InProceedings{Ciresan2012,
  author    = {D. Ciresan and U. Meier and J. Schmidhuber},
  booktitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
  title     = {Multi-column deep neural networks for image classification},
  year      = {2012},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2012.6248110},
}

@InProceedings{Adelson2001,
  author    = {Edward H. Adelson},
  booktitle = {{SPIE} Proceedings},
  title     = {On seeing stuff: the perception of materials by humans and machines},
  year      = {2001},
  editor    = {Bernice E. Rogowitz and Thrasyvoulos N. Pappas},
  month     = {jun},
  publisher = {{SPIE}},
  doi       = {10.1117/12.429489},
}

@Article{Kirillov2018,
  author        = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollár, Piotr},
  title         = {Panoptic Segmentation},
  year          = {2018},
  month         = jan,
  abstract      = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1801.00868},
  eprint        = {1801.00868},
  file          = {:http\://arxiv.org/pdf/1801.00868v3:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@InProceedings{Ciresan2012a,
  author    = {Ciresan, Dan and Giusti, Alessandro and Gambardella, Luca and Schmidhuber, J\"{u}rgen},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
  year      = {2012},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {25},
  url       = {https://proceedings.neurips.cc/paper/2012/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf},
}

@Article{Badrinarayanan2017,
  author    = {Vijay Badrinarayanan and Alex Kendall and Roberto Cipolla},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title     = {{SegNet}: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
  year      = {2017},
  month     = {dec},
  number    = {12},
  pages     = {2481--2495},
  volume    = {39},
  doi       = {10.1109/tpami.2016.2644615},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InCollection{Ronneberger2015,
  author    = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  year      = {2015},
  pages     = {234--241},
  doi       = {10.1007/978-3-319-24574-4_28},
}

@InProceedings{Long2015,
  author    = {Jonathan Long and Evan Shelhamer and Trevor Darrell},
  booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  title     = {Fully convolutional networks for semantic segmentation},
  year      = {2015},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2015.7298965},
}

@InProceedings{Noh2015,
  author    = {Hyeonwoo Noh and Seunghoon Hong and Bohyung Han},
  booktitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
  title     = {Learning Deconvolution Network for Semantic Segmentation},
  year      = {2015},
  month     = {dec},
  publisher = {{IEEE}},
  doi       = {10.1109/iccv.2015.178},
}

@InProceedings{Nair2010,
author = {Nair, Vinod and Hinton, Geoffrey},
year = {2010},
month = {06},
pages = {807-814},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines Vinod Nair},
volume = {27},
journal = {Proceedings of ICML}
}

@TechReport{Lavelle2017,
  author      = {John Lavelle and Rasmus Tonboe and Matilde B. Jensen and Eva Howe},
  institution = {Danish Meteorological Institute},
  title       = {Validation Report for OSI SAF Global Sea Ice Concentration Product OSI-401-b},
  year        = {2017},
  month       = apr,
  number      = {1.2},
}

@Article{Batrak2019,
  author    = {Yurii Batrak and Malte Müller},
  journal   = {Nature Communications},
  title     = {On the warm bias in atmospheric reanalyses induced by the missing snow over Arctic sea-ice},
  year      = {2019},
  month     = {sep},
  number    = {1},
  volume    = {10},
  doi       = {10.1038/s41467-019-11975-3},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{MihaiDaniel2020,
  author    = {Mihai Daniel RADU and Ilona Madalina COSTEA and Valentin Alexandru STAN},
  booktitle = {2020 12th International Conference on Electronics, Computers and Artificial Intelligence ({ECAI})},
  title     = {Automatic Traffic Sign Recognition Artificial Inteligence - Deep Learning Algorithm},
  year      = {2020},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/ecai50035.2020.9223186},
}

@InProceedings{Zeiler2011,
  author    = {Matthew D. Zeiler and Graham W. Taylor and Rob Fergus},
  booktitle = {2011 International Conference on Computer Vision},
  title     = {Adaptive deconvolutional networks for mid and high level feature learning},
  year      = {2011},
  month     = {nov},
  publisher = {{IEEE}},
  doi       = {10.1109/iccv.2011.6126474},
}

@InProceedings{Zeiler2010,
  author    = {Matthew D. Zeiler and Dilip Krishnan and Graham W. Taylor and Rob Fergus},
  booktitle = {2010 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
  title     = {Deconvolutional networks},
  year      = {2010},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2010.5539957},
}

@Article{Fukushima1980,
  author    = {Kunihiko Fukushima},
  journal   = {Biological Cybernetics},
  title     = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  year      = {1980},
  month     = {apr},
  number    = {4},
  pages     = {193--202},
  volume    = {36},
  doi       = {10.1007/bf00344251},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Yu2020a,
  author        = {Yu, Tong and Zhu, Hong},
  title         = {Hyper-Parameter Optimization: A Review of Algorithms and Applications},
  year          = {2020},
  month         = mar,
  abstract      = {Since deep neural networks were developed, they have made huge contributions to everyday lives. Machine learning provides more rational advice than humans are capable of in almost every aspect of daily life. However, despite this achievement, the design and training of neural networks are still challenging and unpredictable procedures. To lower the technical thresholds for common users, automated hyper-parameter optimization (HPO) has become a popular topic in both academic and industrial areas. This paper provides a review of the most essential topics on HPO. The first section introduces the key hyper-parameters related to model training and structure, and discusses their importance and methods to define the value range. Then, the research focuses on major optimization algorithms and their applicability, covering their efficiency and accuracy especially for deep learning networks. This study next reviews major services and toolkits for HPO, comparing their support for state-of-the-art searching algorithms, feasibility with major deep learning frameworks, and extensibility for new modules designed by users. The paper concludes with problems that exist when HPO is applied to deep learning, a comparison between optimization algorithms, and prominent approaches for model evaluation with limited computational resources.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2003.05689},
  eprint        = {2003.05689},
  file          = {:http\://arxiv.org/pdf/2003.05689v1:PDF},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InCollection{Bridle1990,
  author    = {John S. Bridle},
  booktitle = {Neurocomputing},
  publisher = {Springer Berlin Heidelberg},
  title     = {Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition},
  year      = {1990},
  pages     = {227--236},
  doi       = {10.1007/978-3-642-76153-9_28},
}

@Article{Jia2014,
  author        = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  title         = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  year          = {2014},
  month         = jun,
  abstract      = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ($\approx$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1408.5093},
  eprint        = {1408.5093},
  file          = {:http\://arxiv.org/pdf/1408.5093v1:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Simonyan2014,
  author        = {Simonyan, Karen and Zisserman, Andrew},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year          = {2014},
  month         = sep,
  abstract      = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1409.1556},
  eprint        = {1409.1556},
  file          = {:http\://arxiv.org/pdf/1409.1556v6:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Szegedy2014,
  author        = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title         = {Going Deeper with Convolutions},
  year          = {2014},
  month         = sep,
  abstract      = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1409.4842},
  eprint        = {1409.4842},
  file          = {:http\://arxiv.org/pdf/1409.4842v1:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Huang2016,
  author        = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  title         = {Densely Connected Convolutional Networks},
  year          = {2016},
  month         = aug,
  abstract      = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1608.06993},
  eprint        = {1608.06993},
  file          = {:http\://arxiv.org/pdf/1608.06993v5:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Chen2018,
  author    = {Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L. Yuille},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title     = {{DeepLab}: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected {CRFs}},
  year      = {2018},
  month     = {apr},
  number    = {4},
  pages     = {834--848},
  volume    = {40},
  doi       = {10.1109/tpami.2017.2699184},
  file      = {:Chen2018.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Dumont2022,
  author    = {Dany Dumont},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title     = {Marginal ice zone dynamics: history, definitions and research perspectives},
  year      = {2022},
  month     = {sep},
  number    = {2235},
  volume    = {380},
  doi       = {10.1098/rsta.2021.0253},
  publisher = {The Royal Society},
}

@Article{Wu2022,
  author    = {Ming-Yu Wu and Yan Wu and Xin-Yi Yuan and Zhi-Hua Chen and Wei-Tao Wu and Nadine Aubry},
  journal   = {Applied Sciences},
  title     = {Fast Prediction of Flow Field around Airfoils Based on Deep Convolutional Neural Network},
  year      = {2022},
  month     = {nov},
  number    = {23},
  pages     = {12075},
  volume    = {12},
  doi       = {10.3390/app122312075},
  publisher = {{MDPI} {AG}},
}

@Article{Yamashita2018,
  author    = {Rikiya Yamashita and Mizuho Nishio and Richard Kinh Gian Do and Kaori Togashi},
  journal   = {Insights into Imaging},
  title     = {Convolutional neural networks: an overview and application in radiology},
  year      = {2018},
  month     = {jun},
  number    = {4},
  pages     = {611--629},
  volume    = {9},
  doi       = {10.1007/s13244-018-0639-9},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Geman1992,
  author    = {Stuart Geman and Elie Bienenstock and Ren{\'{e}} Doursat},
  journal   = {Neural Computation},
  title     = {Neural Networks and the Bias/Variance Dilemma},
  year      = {1992},
  month     = {jan},
  number    = {1},
  pages     = {1--58},
  volume    = {4},
  doi       = {10.1162/neco.1992.4.1.1},
  publisher = {{MIT} Press - Journals},
}

@Article{BanoMedina2020,
  author    = {Jorge Ba{\~{n}}o-Medina and Rodrigo Manzanas and Jos{\'{e}} Manuel Guti{\'{e}}rrez},
  journal   = {Geoscientific Model Development},
  title     = {Configuration and intercomparison of deep learning neural models for statistical downscaling},
  year      = {2020},
  month     = {apr},
  number    = {4},
  pages     = {2109--2124},
  volume    = {13},
  doi       = {10.5194/gmd-13-2109-2020},
  file      = {:BanoMedina2020.pdf:PDF},
  publisher = {Copernicus {GmbH}},
}

@Article{Qiao2019,
  author        = {Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
  title         = {Micro-Batch Training with Batch-Channel Normalization and Weight Standardization},
  year          = {2019},
  month         = mar,
  abstract      = {Batch Normalization (BN) has become an out-of-box technique to improve deep network training. However, its effectiveness is limited for micro-batch training, i.e., each GPU typically has only 1-2 images for training, which is inevitable for many computer vision tasks, e.g., object detection and semantic segmentation, constrained by memory consumption. To address this issue, we propose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to bring two success factors of BN into micro-batch training: 1) the smoothing effects on the loss landscape and 2) the ability to avoid harmful elimination singularities along the training trajectory. WS standardizes the weights in convolutional layers to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients; BCN combines batch and channel normalizations and leverages estimated statistics of the activations in convolutional layers to keep networks away from elimination singularities. We validate WS and BCN on comprehensive computer vision tasks, including image classification, object detection, instance segmentation, video recognition and semantic segmentation. All experimental results consistently show that WS and BCN improve micro-batch training significantly. Moreover, using WS and BCN with micro-batch training is even able to match or outperform the performances of BN with large-batch training.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1903.10520},
  eprint        = {1903.10520},
  file          = {:http\://arxiv.org/pdf/1903.10520v2:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Paszke2019,
  author        = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  title         = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year          = {2019},
  month         = dec,
  abstract      = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1912.01703},
  eprint        = {1912.01703},
  file          = {:http\://arxiv.org/pdf/1912.01703v1:PDF},
  keywords      = {Machine Learning (cs.LG), Mathematical Software (cs.MS), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Misc{Snow2022,
  author    = {Snow, Alan D. and Whitaker, Jeff and Cochran, Micah and Van Den Bossche, Joris and Mayo, Chris and Miara, Idan and Cochrane, Paul and De Kloe, Jos and Karney, Charles and {, Filipe} and Couwenberg, Bas and Lostis, Guillaume and Dearing, Justin and Ouzounoudis, George and Jurd, Brendan and Gohlke, Christoph and Hoese, David and Itkin, Mikhail and May, Ryan and Little, Bill and {, Heitor} and Shadchin, Alexander and Wiedemann, Bernhard M. and Barker, Chris and Willoughby, Chris and {DWesl} and Hemberger, Dan and Haberthür, David and Popov, Eduard},
  title     = {pyproj4/pyproj: 3.4.1 Release},
  year      = {2022},
  copyright = {Open Access},
  doi       = {10.5281/ZENODO.2592232},
  publisher = {Zenodo},
}

@Article{Obite2020,
  author    = {C. P. Obite and N. P. Olewuezi and G. U. Ugwuanyim and D. C. Bartholomew},
  journal   = {Asian Journal of Probability and Statistics},
  title     = {Multicollinearity Effect in Regression Analysis: A Feed Forward Artificial Neural Network Approach},
  year      = {2020},
  month     = {jan},
  pages     = {22--33},
  doi       = {10.9734/ajpas/2020/v6i130151},
  publisher = {Sciencedomain International},
}

@Article{Chan2022,
  author    = {Jireh Yi-Le Chan and Steven Mun Hong Leow and Khean Thye Bea and Wai Khuen Cheng and Seuk Wai Phoong and Zeng-Wei Hong and Yen-Lin Chen},
  journal   = {Mathematics},
  title     = {Mitigating the Multicollinearity Problem and Its Machine Learning Approach: A Review},
  year      = {2022},
  month     = {apr},
  number    = {8},
  pages     = {1283},
  volume    = {10},
  doi       = {10.3390/math10081283},
  publisher = {{MDPI} {AG}},
}

@Article{Vinogradova2020,
  author    = {Kira Vinogradova and Alexandr Dibrov and Gene Myers},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {Towards Interpretable Semantic Segmentation via Gradient-Weighted Class Activation Mapping (Student Abstract)},
  year      = {2020},
  month     = {apr},
  number    = {10},
  pages     = {13943--13944},
  volume    = {34},
  doi       = {10.1609/aaai.v34i10.7244},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
}

@Article{Selvaraju2016,
  author        = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  journal       = {International Journal of Computer Vision},
  title         = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
  year          = {2016},
  month         = {oct},
  number        = {2},
  pages         = {336--359},
  volume        = {128},
  abstract      = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  date          = {2016-10-07},
  doi           = {10.1007/s11263-019-01228-7},
  eprint        = {1610.02391},
  file          = {:http\://arxiv.org/pdf/1610.02391v4:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {Springer Science and Business Media {LLC}},
}

@Article{Adadi2018,
  author    = {Amina Adadi and Mohammed Berrada},
  journal   = {{IEEE} Access},
  title     = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence ({XAI})},
  year      = {2018},
  pages     = {52138--52160},
  volume    = {6},
  doi       = {10.1109/access.2018.2870052},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Lopes2022,
  author    = {Pedro Lopes and Eduardo Silva and Cristiana Braga and Tiago Oliveira and Lu{\'{\i}}s Rosado},
  journal   = {Applied Sciences},
  title     = {{XAI} Systems Evaluation: A Review of Human and Computer-Centred Methods},
  year      = {2022},
  month     = {sep},
  number    = {19},
  pages     = {9423},
  volume    = {12},
  doi       = {10.3390/app12199423},
  publisher = {{MDPI} {AG}},
}

@InProceedings{Lundberg2017,
  author    = {Lundberg, Scott M and Lee, Su-In},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {A Unified Approach to Interpreting Model Predictions},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
}

@Article{Sundararajan2017,
  author        = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  title         = {Axiomatic Attribution for Deep Networks},
  year          = {2017},
  month         = mar,
  abstract      = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1703.01365},
  eprint        = {1703.01365},
  file          = {:http\://arxiv.org/pdf/1703.01365v2:PDF},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Simonyan2013,
  author        = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  title         = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  year          = {2013},
  month         = dec,
  abstract      = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1312.6034},
  eprint        = {1312.6034},
  file          = {:http\://arxiv.org/pdf/1312.6034v2:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@InProceedings{Zhou2016,
  author    = {Bolei Zhou and Aditya Khosla and Agata Lapedriza and Aude Oliva and Antonio Torralba},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  title     = {Learning Deep Features for Discriminative Localization},
  year      = {2016},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2016.319},
}

@Article{Linardatos2020,
  author    = {Pantelis Linardatos and Vasilis Papastefanopoulos and Sotiris Kotsiantis},
  journal   = {Entropy},
  title     = {Explainable {AI}: A Review of Machine Learning Interpretability Methods},
  year      = {2020},
  month     = {dec},
  number    = {1},
  pages     = {18},
  volume    = {23},
  doi       = {10.3390/e23010018},
  publisher = {{MDPI} {AG}},
}

@Article{Barry1993,
  author    = {R. G. Barry and M. C. Serreze and J. A. Maslanik and R. H. Preller},
  journal   = {Reviews of Geophysics},
  title     = {The Arctic Sea Ice-Climate System: Observations and modeling},
  year      = {1993},
  number    = {4},
  pages     = {397},
  volume    = {31},
  doi       = {10.1029/93rg01998},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Colony1984,
  author    = {R. Colony and A. S. Thorndike},
  journal   = {Journal of Geophysical Research},
  title     = {An estimate of the mean field of Arctic sea ice motion},
  year      = {1984},
  number    = {C6},
  pages     = {10623},
  volume    = {89},
  doi       = {10.1029/jc089ic06p10623},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Kaur2018,
  author    = {Satwant Kaur and Jens K. Ehn and David G. Barber},
  journal   = {Polar Record},
  title     = {Pan-arctic winter drift speeds and changing patterns of sea ice motion: 1979{\textendash}2015},
  year      = {2018},
  month     = {sep},
  number    = {5-6},
  pages     = {303--311},
  volume    = {54},
  doi       = {10.1017/s0032247418000566},
  publisher = {Cambridge University Press ({CUP})},
}

@Misc{Kreiner2023,
  author    = {Kreiner, Matilde Brandt and Wulf, Tore and Jakobsen, Jens and Nielsen, Allan Aasbjerg and Pedersen, Leif Toudal},
  title     = {Poster: Inter- and intra-analyst ice edge assessment},
  year      = {2023},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.6084/M9.FIGSHARE.22312648.V1},
  keywords  = {Other earth sciences not elsewhere classified},
  publisher = {figshare},
}

@Article{Strong2012,
  author    = {Courtenay Strong},
  journal   = {Climate Dynamics},
  title     = {Atmospheric influence on Arctic marginal ice zone position and width in the Atlantic sector, February{\textendash}April 1979{\textendash}2010},
  year      = {2012},
  month     = {may},
  number    = {12},
  pages     = {3091--3102},
  volume    = {39},
  doi       = {10.1007/s00382-012-1356-6},
  publisher = {Springer Science and Business Media {LLC}},
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
} 

@Article{Araujo2019,
  author    = {Andr{\&}eacute$\mathsemicolon$ Araujo and Wade Norris and Jack Sim},
  journal   = {Distill},
  title     = {Computing Receptive Fields of Convolutional Neural Networks},
  year      = {2019},
  month     = {nov},
  number    = {11},
  volume    = {4},
  doi       = {10.23915/distill.00021},
  publisher = {Distill Working Group},
}

@InProceedings{Graves2013,
  author    = {Alex Graves and Abdel-rahman Mohamed and Geoffrey Hinton},
  booktitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
  title     = {Speech recognition with deep recurrent neural networks},
  year      = {2013},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icassp.2013.6638947},
}

@Article{Luo2017,
  author        = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  title         = {Understanding the Effective Receptive Field in Deep Convolutional Neural Networks},
  year          = {2017},
  month         = jan,
  abstract      = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1701.04128},
  eprint        = {1701.04128},
  file          = {:http\://arxiv.org/pdf/1701.04128v2:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Kingma2014,
  author        = {Kingma, Diederik P. and Ba, Jimmy},
  title         = {Adam: A Method for Stochastic Optimization},
  year          = {2014},
  month         = dec,
  abstract      = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1412.6980},
  eprint        = {1412.6980},
  file          = {:http\://arxiv.org/pdf/1412.6980v9:PDF},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Shorten2019,
  author    = {Connor Shorten and Taghi M. Khoshgoftaar},
  journal   = {Journal of Big Data},
  title     = {A survey on Image Data Augmentation for Deep Learning},
  year      = {2019},
  month     = {jul},
  number    = {1},
  volume    = {6},
  doi       = {10.1186/s40537-019-0197-0},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Williams2013,
  author    = {Timothy D. Williams and Luke G. Bennetts and Vernon A. Squire and Dany Dumont and Laurent Bertino},
  journal   = {Ocean Modelling},
  title     = {Wave{\textendash}ice interactions in the marginal ice zone. Part 1: Theoretical foundations},
  year      = {2013},
  month     = {nov},
  pages     = {81--91},
  volume    = {71},
  doi       = {10.1016/j.ocemod.2013.05.010},
  file      = {:Williams2013.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{Boutin2020,
  author    = {Guillaume Boutin and Camille Lique and Fabrice Ardhuin and Cl{\'{e}}ment Rousset and Claude Talandier and Mickael Accensi and Fanny Girard-Ardhuin},
  journal   = {The Cryosphere},
  title     = {Towards a coupled model to investigate wave{\textendash}sea ice interactions in the Arctic marginal ice zone},
  year      = {2020},
  month     = {mar},
  number    = {2},
  pages     = {709--735},
  volume    = {14},
  doi       = {10.5194/tc-14-709-2020},
  file      = {:Boutin2020.pdf:PDF},
  publisher = {Copernicus {GmbH}},
}

@Article{Liu2018,
  author        = {Liu, Guilin and Reda, Fitsum A. and Shih, Kevin J. and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
  title         = {Image Inpainting for Irregular Holes Using Partial Convolutions},
  year          = {2018},
  month         = apr,
  abstract      = {Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1804.07723},
  eprint        = {1804.07723},
  file          = {:http\://arxiv.org/pdf/1804.07723v2:PDF},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@InProceedings{Jha2020,
  author    = {Ankit Jha and Awanish Kumar and Shivam Pande and Biplab Banerjee and Subhasis Chaudhuri},
  booktitle = {2020 {IEEE} International Conference on Image Processing ({ICIP})},
  title     = {{MT}-{UNET}: A Novel U-Net Based Multi-Task Architecture For Visual Scene Understanding},
  year      = {2020},
  month     = {oct},
  publisher = {{IEEE}},
  doi       = {10.1109/icip40778.2020.9190695},
}

@Article{Crawshaw2020,
  author        = {Crawshaw, Michael},
  title         = {Multi-Task Learning with Deep Neural Networks: A Survey},
  year          = {2020},
  month         = sep,
  abstract      = {Multi-task learning (MTL) is a subfield of machine learning in which multiple tasks are simultaneously learned by a shared model. Such approaches offer advantages like improved data efficiency, reduced overfitting through shared representations, and fast learning by leveraging auxiliary information. However, the simultaneous learning of multiple tasks presents new design and optimization challenges, and choosing which tasks should be learned jointly is in itself a non-trivial problem. In this survey, we give an overview of multi-task learning methods for deep neural networks, with the aim of summarizing both the well-established and most recent directions within the field. Our discussion is structured according to a partition of the existing deep MTL techniques into three groups: architectures, optimization methods, and task relationship learning. We also provide a summary of common multi-task benchmarks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2009.09796},
  eprint        = {2009.09796},
  file          = {:http\://arxiv.org/pdf/2009.09796v1:PDF},
  keywords      = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InCollection{Zhang2014,
  author    = {Zhanpeng Zhang and Ping Luo and Chen Change Loy and Xiaoou Tang},
  booktitle = {Computer Vision {\textendash} {ECCV} 2014},
  publisher = {Springer International Publishing},
  title     = {Facial Landmark Detection by Deep Multi-task Learning},
  year      = {2014},
  pages     = {94--108},
  doi       = {10.1007/978-3-319-10599-4_7},
}

@Article{DeVries2018,
  author        = {DeVries, Terrance and Taylor, Graham W.},
  title         = {Learning Confidence for Out-of-Distribution Detection in Neural Networks},
  year          = {2018},
  month         = feb,
  abstract      = {Modern neural networks are very powerful predictive models, but they are often incapable of recognizing when their predictions may be wrong. Closely related to this is the task of out-of-distribution detection, where a network must determine whether or not an input is outside of the set on which it is expected to safely perform. To jointly address these issues, we propose a method of learning confidence estimates for neural networks that is simple to implement and produces intuitively interpretable outputs. We demonstrate that on the task of out-of-distribution detection, our technique surpasses recently proposed techniques which construct confidence based on the network's output distribution, without requiring any additional labels or access to out-of-distribution examples. Additionally, we address the problem of calibrating out-of-distribution detectors, where we demonstrate that misclassified in-distribution examples can be used as a proxy for out-of-distribution examples.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1802.04865},
  eprint        = {1802.04865},
  file          = {:http\://arxiv.org/pdf/1802.04865v1:PDF},
  keywords      = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {stat.ML},
  publisher     = {arXiv},
}

@TechReport{Carrasco2022,
  author = {Ana Carrasco and Øyvind Saetra and Arild Burud and Malte Müller and Arne Melsom},
  title  = {PRODUCT USER MANUAL For Arctic Ocean Wave Analysis and Forecasting Products ARCTIC\_ANALYSIS\_FORECAST\_WAV\_002\_014},
  year   = {2022},
}

@Article{Mueller2023,
  author = {Malte Müller and Yurii Batrak and Frode Dinessen and Rafael Grote and Keguang Wang},
  title  = {"Submitted" Challenges in the description of sea-ice for a kilometer-scale weather forecasting system},
  year   = {2023},
}

@Article{Murphy1993,
  author    = {Allan H. Murphy},
  journal   = {Weather and Forecasting},
  title     = {What Is a Good Forecast? An Essay on the Nature of Goodness in Weather Forecasting},
  year      = {1993},
  month     = {jun},
  number    = {2},
  pages     = {281--293},
  volume    = {8},
  doi       = {10.1175/1520-0434(1993)008<0281:wiagfa>2.0.co;2},
  publisher = {American Meteorological Society},
}

@Comment{jabref-meta: databaseType:bibtex;}

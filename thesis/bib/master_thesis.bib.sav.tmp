@InProceedings{Nair2010,
  author    = {Vinod Nair and Geoffrey E. Hinton},
  booktitle = {ICML},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  year      = {2010},
  pages     = {807-814},
  cdate     = {1262304000000},
  comment   = {Paper presenting the ReLU activation function},
  url       = {https://icml.cc/Conferences/2010/papers/432.pdf},
}

@Article{Palerme2021,
  author    = {Cyril Palerme and Malte Müller},
  title     = {Calibration of sea ice drift forecasts using random forest algorithms},
  year      = {2021},
  month     = {aug},
  number    = {8},
  pages     = {3989--4004},
  volume    = {15},
  doi       = {10.5194/tc-15-3989-2021},
  file      = {:palerme2021.pdf:PDF},
  publisher = {Copernicus {GmbH}},
}

@Article{Fritzner2020,
  author    = {Sindre Fritzner and Rune Graversen and Kai H. Christensen},
  title     = {Assessment of High-Resolution Dynamical and Machine Learning Models for Prediction of Sea Ice Concentration in a Regional Application},
  year      = {2020},
  month     = oct,
  note      = {Neural Networks for predicting Sea-Ice concentration are only slightly more accurate than persistence forecasting for short-term predictions.},
  number    = {11},
  volume    = {125},
  comment   = {*Read*},
  doi       = {10.1029/2020jc016277},
  file      = {:fritzner2020.pdf:PDF},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Soenderby2020,
  author        = {Casper Kaae Sønderby and Lasse Espeholt and Jonathan Heek and Mostafa Dehghani and Avital Oliver and Tim Salimans and Shreya Agrawal and Jason Hickey and Nal Kalchbrenner},
  title         = {MetNet: A Neural Weather Model for Precipitation Forecasting},
  year          = {2020},
  month         = mar,
  abstract      = {Weather forecasting is a long standing scientific challenge with direct social and economic impact. The task is suitable for deep neural networks due to vast amounts of continuously collected data and a rich spatial and temporal structure that presents long range dependencies. We introduce MetNet, a neural network that forecasts precipitation up to 8 hours into the future at the high spatial resolution of 1 km$^2$ and at the temporal resolution of 2 minutes with a latency in the order of seconds. MetNet takes as input radar and satellite data and forecast lead time and produces a probabilistic precipitation map. The architecture uses axial self-attention to aggregate the global context from a large input patch corresponding to a million square kilometers. We evaluate the performance of MetNet at various precipitation thresholds and find that MetNet outperforms Numerical Weather Prediction at forecasts of up to 7 to 8 hours on the scale of the continental United States.},
  archiveprefix = {arXiv},
  eprint        = {2003.12140},
  file          = {:http\://arxiv.org/pdf/2003.12140v2:PDF},
  keywords      = {cs.LG, physics.ao-ph, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Agrawal2019,
  author        = {Shreya Agrawal and Luke Barrington and Carla Bromberg and John Burge and Cenk Gazen and Jason Hickey},
  title         = {Machine Learning for Precipitation Nowcasting from Radar Images},
  year          = {2019},
  month         = dec,
  abstract      = {High-resolution nowcasting is an essential tool needed for effective adaptation to climate change, particularly for extreme weather. As Deep Learning (DL) techniques have shown dramatic promise in many domains, including the geosciences, we present an application of DL to the problem of precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1 hour) predictions of precipitation. We treat forecasting as an image-to-image translation problem and leverage the power of the ubiquitous UNET convolutional neural network. We find this performs favorably when compared to three commonly used models: optical flow, persistence and NOAA's numerical one-hour HRRR nowcasting prediction.},
  archiveprefix = {arXiv},
  eprint        = {1912.12132},
  file          = {:http\://arxiv.org/pdf/1912.12132v1:PDF},
  keywords      = {cs.CV, cs.LG, stat.ML},
  primaryclass  = {cs.CV},
}

@Article{Ravuri2021,
  author    = {Suman Ravuri and Karel Lenc and Matthew Willson and Dmitry Kangin and Remi Lam and Piotr Mirowski and Megan Fitzsimons and Maria Athanassiadou and Sheleem Kashem and Sam Madge and Rachel Prudden and Amol Mandhane and Aidan Clark and Andrew Brock and Karen Simonyan and Raia Hadsell and Niall Robinson and Ellen Clancy and Alberto Arribas and Shakir Mohamed},
  journal   = {Nature},
  title     = {Skilful precipitation nowcasting using deep generative models of radar},
  year      = {2021},
  month     = {sep},
  number    = {7878},
  pages     = {672--677},
  volume    = {597},
  doi       = {10.1038/s41586-021-03854-z},
  file      = {:ravuri2021.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Schultz2021,
  author    = {M. G. Schultz and C. Betancourt and B. Gong and F. Kleinert and M. Langguth and L. H. Leufen and A. Mozaffari and S. Stadtler},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title     = {Can deep learning beat numerical weather prediction?},
  year      = {2021},
  month     = {feb},
  number    = {2194},
  pages     = {20200097},
  volume    = {379},
  comment   = {*READ*

Section 4 covers a lot of interesting problems surrounding meteorological data, and supplies papers which discusses these "shortcomings" in a ML context. Great paper to use for finding articles which discusses individual problems in-depth

Section 5 touch upon data preparation and model evaluation, especially data splitting with regards to auto-correlation. Also, sources discussing more suitable verification metrics than MSE due to the spatio-temporal correlation of meteorological data are referred to.},
  doi       = {10.1098/rsta.2020.0097},
  file      = {:schultz2021.pdf:PDF},
  publisher = {The Royal Society},
}

@Article{Espeholt2021,
  author        = {Lasse Espeholt and Shreya Agrawal and Casper Sønderby and Manoj Kumar and Jonathan Heek and Carla Bromberg and Cenk Gazen and Jason Hickey and Aaron Bell and Nal Kalchbrenner},
  title         = {Skillful Twelve Hour Precipitation Forecasts using Large Context Neural Networks},
  year          = {2021},
  month         = nov,
  note          = {Met-Net 2},
  abstract      = {The problem of forecasting weather has been scientifically studied for centuries due to its high impact on human lives, transportation, food production and energy management, among others. Current operational forecasting models are based on physics and use supercomputers to simulate the atmosphere to make forecasts hours and days in advance. Better physics-based forecasts require improvements in the models themselves, which can be a substantial scientific challenge, as well as improvements in the underlying resolution, which can be computationally prohibitive. An emerging class of weather models based on neural networks represents a paradigm shift in weather forecasting: the models learn the required transformations from data instead of relying on hand-coded physics and are computationally efficient. For neural models, however, each additional hour of lead time poses a substantial challenge as it requires capturing ever larger spatial contexts and increases the uncertainty of the prediction. In this work, we present a neural network that is capable of large-scale precipitation forecasting up to twelve hours ahead and, starting from the same atmospheric state, the model achieves greater skill than the state-of-the-art physics-based models HRRR and HREF that currently operate in the Continental United States. Interpretability analyses reinforce the observation that the model learns to emulate advanced physics principles. These results represent a substantial step towards establishing a new paradigm of efficient forecasting with neural networks.},
  archiveprefix = {arXiv},
  eprint        = {2111.07470},
  file          = {:http\://arxiv.org/pdf/2111.07470v1:PDF;:espeholt2021.pdf:PDF},
  keywords      = {cs.LG, physics.ao-ph},
  primaryclass  = {cs.LG},
}

@Article{Chantry2021,
  author    = {Matthew Chantry and Hannah Christensen and Peter Dueben and Tim Palmer},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title     = {Opportunities and challenges for machine learning in weather and climate modelling: hard, medium and soft {AI}},
  year      = {2021},
  month     = {feb},
  number    = {2194},
  pages     = {20200083},
  volume    = {379},
  doi       = {10.1098/rsta.2020.0083},
  file      = {:chantry2021.pdf:PDF},
  publisher = {The Royal Society},
}

@Article{Andersson2021,
  author    = {Tom R. Andersson and J. Scott Hosking and Mar{\'{\i}}a P{\'{e}}rez-Ortiz and Brooks Paige and Andrew Elliott and Chris Russell and Stephen Law and Daniel C. Jones and Jeremy Wilkinson and Tony Phillips and James Byrne and Steffen Tietsche and Beena Balan Sarojini and Eduardo Blanchard-Wrigglesworth and Yevgeny Aksenov and Rod Downie and Emily Shuckburgh},
  journal   = {Nature Communications},
  title     = {Seasonal Arctic sea ice forecasting with probabilistic deep learning},
  year      = {2021},
  month     = {aug},
  number    = {1},
  volume    = {12},
  comment   = {Unet for seasonal prediction},
  doi       = {10.1038/s41467-021-25257-4},
  file      = {:Andersson2021.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Zhu2017,
  author    = {Xiao Xiang Zhu and Devis Tuia and Lichao Mou and Gui-Song Xia and Liangpei Zhang and Feng Xu and Friedrich Fraundorfer},
  journal   = {{IEEE} Geoscience and Remote Sensing Magazine},
  title     = {Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources},
  year      = {2017},
  month     = {dec},
  number    = {4},
  pages     = {8--36},
  volume    = {5},
  comment   = {CNN implementation for recognizing spatial features in satellite images},
  doi       = {10.1109/mgrs.2017.2762307},
  file      = {:Zhu2017.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Shi2015,
  author        = {Xingjian Shi and Zhourong Chen and Hao Wang and Dit-Yan Yeung and Wai-kin Wong and Wang-chun Woo},
  title         = {Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting},
  year          = {2015},
  month         = jun,
  abstract      = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
  archiveprefix = {arXiv},
  eprint        = {1506.04214},
  file          = {:http\://arxiv.org/pdf/1506.04214v2:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Wandel2020,
  author        = {Nils Wandel and Michael Weinmann and Reinhard Klein},
  title         = {Learning Incompressible Fluid Dynamics from Scratch -- Towards Fast, Differentiable Fluid Models that Generalize},
  year          = {2020},
  month         = jun,
  abstract      = {Fast and stable fluid simulations are an essential prerequisite for applications ranging from computer-generated imagery to computer-aided design in research and development. However, solving the partial differential equations of incompressible fluids is a challenging task and traditional numerical approximation schemes come at high computational costs. Recent deep learning based approaches promise vast speed-ups but do not generalize to new fluid domains, require fluid simulation data for training, or rely on complex pipelines that outsource major parts of the fluid simulation to traditional methods. In this work, we propose a novel physics-constrained training approach that generalizes to new fluid domains, requires no fluid simulation data, and allows convolutional neural networks to map a fluid state from time-point t to a subsequent state at time t + dt in a single forward pass. This simplifies the pipeline to train and evaluate neural fluid models. After training, the framework yields models that are capable of fast fluid simulations and can handle various fluid phenomena including the Magnus effect and Karman vortex streets. We present an interactive real-time demo to show the speed and generalization capabilities of our trained models. Moreover, the trained neural networks are efficient differentiable fluid solvers as they offer a differentiable update step to advance the fluid simulation in time. We exploit this fact in a proof-of-concept optimal control experiment. Our models significantly outperform a recent differentiable fluid solver in terms of computational speed and accuracy.},
  archiveprefix = {arXiv},
  comment       = {Unsupervised learning scheme, maybe CNN, to solve Navier Stokes equations, simulating incompressible fluid motion.},
  eprint        = {2006.08762},
  file          = {:http\://arxiv.org/pdf/2006.08762v3:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Cirstea2018,
  author        = {Razvan-Gabriel Cirstea and Darius-Valer Micu and Gabriel-Marcel Muresan and Chenjuan Guo and Bin Yang},
  title         = {Correlated Time Series Forecasting using Deep Neural Networks: A Summary of Results},
  year          = {2018},
  month         = aug,
  abstract      = {Cyber-physical systems often consist of entities that interact with each other over time. Meanwhile, as part of the continued digitization of industrial processes, various sensor technologies are deployed that enable us to record time-varying attributes (a.k.a., time series) of such entities, thus producing correlated time series. To enable accurate forecasting on such correlated time series, this paper proposes two models that combine convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The first model employs a CNN on each individual time series, combines the convoluted features, and then applies an RNN on top of the convoluted features in the end to enable forecasting. The second model adds additional auto-encoders into the individual CNNs, making the second model a multi-task learning model, which provides accurate and robust forecasting. Experiments on two real-world correlated time series data set suggest that the proposed two models are effective and outperform baselines in most settings. This report extends the paper "Correlated Time Series Forecasting using Multi-Task Deep Neural Networks," to appear in ACM CIKM 2018, by providing additional experimental results.},
  archiveprefix = {arXiv},
  comment       = {Applying deep learning to highly spatially -and temporally correlated data, such as meterological data. Though the paper itself is general, i.e. not applied to met-data.},
  eprint        = {1808.09794},
  file          = {:http\://arxiv.org/pdf/1808.09794v2:PDF;:Cirstea2018.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Book{Maraun_2017,
  author    = {Douglas Maraun and Martin Widmann},
  publisher = {Cambridge University Press},
  title     = {Statistical Downscaling and Bias Correction for Climate Research},
  year      = {2017},
  address   = {Cambridge},
  isbn      = {9781107588783},
  month     = {dec},
  doi       = {10.1017/9781107588783},
  file      = {:Maraun2017.pdf:PDF},
}

@Misc{MOI2021,
  author    = {{Mercator Ocean International}},
  title     = {Arctic Ocean - High resolution Sea Ice Concentration and Sea Ice Type},
  year      = {2021},
  doi       = {10.48670/MOI-00122},
  file      = {:Dinessen2021.pdf:PDF},
  keywords  = {oceanography},
  language  = {en},
  publisher = {Mercator Ocean International},
}

@Misc{MOI2015,
  author    = {Frode Dinessen and Bruce Hackett and Matilde Brandt Kreiner},
  title     = {Arctic Ocean - Sea Ice Concentration Charts - Svalbard and Greenland},
  year      = {2015},
  doi       = {10.48670/MOI-00128},
  file      = {:Dinessen2020.pdf:PDF},
  keywords  = {oceanography},
  language  = {en},
  publisher = {Mercator Ocean International},
}

@Article{Ronneberger2015,
  author        = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  title         = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  year          = {2015},
  month         = may,
  abstract      = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  eprint        = {1505.04597},
  file          = {:http\://arxiv.org/pdf/1505.04597v1:PDF;:Ronneberger2015.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Misc{JETSI2014,
  author    = {{JCOMM Expert Team on Sea Ice}},
  title     = {Sea-Ice Nomenclature: snapshot of the WMO Sea Ice Nomenclature WMO No. 259, volume 1 – Terminology and Codes; Volume II – Illustrated Glossary and III – International System of Sea-Ice Symbols) .},
  year      = {2014},
  doi       = {10.25607/OBP-1515},
  file      = {:Sea_Ice_Nomenclature_March_2014.pdf:PDF},
  publisher = {WMO-JCOMM},
}

@Article{Palerme2019,
  author    = {Cyril Palerme and Malte Müller and Arne Melsom},
  journal   = {Geophysical Research Letters},
  title     = {An Intercomparison of Verification Scores for Evaluating the Sea Ice Edge Position in Seasonal Forecasts},
  year      = {2019},
  month     = {may},
  number    = {9},
  pages     = {4757--4763},
  volume    = {46},
  doi       = {10.1029/2019gl082482},
  file      = {Sea Ice Edge position verification for seasonal forecasting:Palerme2019.pdf:PDF},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Melsom2019,
  author    = {Arne Melsom and Cyril Palerme and Malte Müller},
  journal   = {Ocean Science},
  title     = {Validation metrics for ice edge position forecasts},
  year      = {2019},
  month     = {may},
  number    = {3},
  pages     = {615--630},
  volume    = {15},
  doi       = {10.5194/os-15-615-2019},
  file      = {Validation metrics for ice edge position forecasts:Melsom2019.pdf:PDF},
  publisher = {Copernicus {GmbH}},
}

@Article{Goessling2016,
  author    = {H. F. Goessling and S. Tietsche and J. J. Day and E. Hawkins and T. Jung},
  journal   = {Geophysical Research Letters},
  title     = {Predictability of the Arctic sea ice edge},
  year      = {2016},
  month     = {feb},
  number    = {4},
  pages     = {1642--1650},
  volume    = {43},
  doi       = {10.1002/2015gl067232},
  file      = {Article that introduces IIEE:Goessling2016.pdf:PDF},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Goessling2018,
  author    = {H. F. Goessling and T. Jung},
  journal   = {Quarterly Journal of the Royal Meteorological Society},
  title     = {A probabilistic verification score for contours: Methodology and application to Arctic ice-edge forecasts},
  year      = {2018},
  month     = apr,
  number    = {712},
  pages     = {735--743},
  volume    = {144},
  doi       = {10.1002/qj.3242},
  file      = {Article that introduces SPS:Goessling2018.pdf:PDF},
  publisher = {Wiley},
}

@Article{Micikevicius2017,
  author        = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
  title         = {Mixed Precision Training},
  year          = {2017},
  month         = oct,
  abstract      = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
  archiveprefix = {arXiv},
  comment       = {Paper presenting mixed precision training. Mixed precision was considered for use in the thesis during model training, as it decreases the memory needed for a training sample, speed up computations as well as decreases the memory bandwidth. However, mixed precision proved to be more difficult to implement than the docs suggested. With unclear descriptions and guides pointing towards different direction. Thus mixed precision was not pursued further, as the nvidia A100 GPU with 80GB of memory is able to fit a 1km model without decreasing sample size.},
  eprint        = {1710.03740},
  file          = {:http\://arxiv.org/pdf/1710.03740v3:PDF},
  keywords      = {cs.AI, cs.LG, stat.ML},
  primaryclass  = {cs.AI},
}

@Misc{TwoSigma,
  author       = {Two Sigma},
  howpublished = {webpage},
  title        = {A Workaround for Non-Determinism in TensorFlow},
  comment      = {Web-article discussing some workarounds for non-deterministic behaviour resulting from training models on a GPU},
}

@Article{Casati2008,
  author    = {B. Casati and L. J. Wilson and D. B. Stephenson and P. Nurmi and A. Ghelli and M. Pocernich and U. Damrath and E. E. Ebert and B. G. Brown and S. Mason},
  journal   = {Meteorological Applications},
  title     = {Forecast verification: current status and future directions},
  year      = {2008},
  number    = {1},
  pages     = {3--18},
  volume    = {15},
  doi       = {10.1002/met.52},
  publisher = {Wiley},
}

@Article{Dukhovskoy2015,
  author    = {Dmitry S. Dukhovskoy and Jonathan Ubnoske and Edward Blanchard-Wrigglesworth and Hannah R. Hiester and Andrey Proshutinsky},
  journal   = {Journal of Geophysical Research: Oceans},
  title     = {Skill metrics for evaluation and comparison of sea ice models},
  year      = {2015},
  month 
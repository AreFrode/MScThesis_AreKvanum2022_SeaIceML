\documentclass[../main/thesis.tex]{subfiles}

% Overleaf graphics path
\graphicspath{{thesis/methodology/figures/}}

% Local graphics path
% \graphicspath{{/home/arefk/uio/MScThesis_AreKvanum2022_SeaIceML/thesis/methodology/figures/}}


\begin{document}
\section{Methodological framework}
\label{sec:methodology}
This section will first outline the theoretical background of convolutions from a deep learning point of view, as well as provide a brief overview of image segmentation as a computer vision task. Second, the methodological framework of the U-Net architecture, the deep neural network which is used in the present work, is outlined with a detailed description of its training loop and central algorithms. Thirdly, validation metrics which are used to asses the performance of the developed deep learning system will be described. Finally, aspects of explainable Artificial Intelligence (AI) will be explored in terms of understanding how a deep learning system make a single decision. 

\subsection{Convolutional layers}
\label{sec:convolutional-layer}
Convolutional layers incorporated into a deep neural network which are utilizing the backpropagation algorithm \citep{Rumelhart1986} was initially proposed by \citet{LeCun1989} to classify handwritten numbers. The layer \citet{LeCun1989} presented consists of an arbitrary amount of filters, which are small two dimensional matrices (e.g. $(3 \times 3)$ pixels) designed to capture a certain structure in the image such as lines or edges. Each filter contains trainable weights, which are learned from the data during backpropagation \citep{LeCun1989} and gradient descent. When a filter is convolved with all possible local neighborhoods from the input, it outputs a feature map which represents where the input image triggered a response from the filter \citep{Zeiler2010}. Moreover, inputting feature maps to a convolutional layer allows for the filters to respond to combinations of lower level structures, which trains the layer to detect more complicated patterns \citep{Fukushima1980}. Additionally, stacking convolutional layers in a network-architecture structure increases the field of view for each subsequent layer, allowing each layer to observe an increasingly complex pattern of higher order feature maps at increasingly larger spatial scales \citep{Fukushima1980}. As a result, convolutional layers are able to discern between object and background as they perceive only a limited view of the scene. The convolutional layer is also invariant to the translation of the object, since the filter is constant when creating the feature map, i.e. the filter is detecting the same feature at all locations in the image, known as weight sharing \citep{LeCun1989}.

The number of trainable parameters for a convolutional layer is equal to the size of a filter times the number of filters. As a result, the number of trainable parameters is invariant to the spatial extent of the input images. Contrarily, fitting a fully connected layer to spatial gridded data consists of associating a separate trainable parameter to each pixel. As such, the size of a fully connected layer scales with the size of the image, which increases the risk of overfitting the network. In the case of the convolutional layer, \citet{LeCun1989} notes that reducing the number of trainable parameters through weight sharing constrains the solution space such that overfitting is avoided while still having enough trainable parameters to fit the layer to the data. Furthermore, the fully connected layer is not invariant to translation as each trainable parameter is exclusive to their respective pixel, hence no weight sharing. As such, the layer is unable to detect a similar object at a different position, reducing their usefulness for image-based prediction tasks.

Finally, \citet{Ciresan2012} showed that the processing time of a convolutional layer is significantly shortened by utilizing a graphics processing unit (GPU), due to their large amount of compute cores compared to traditional Central Processing Units (CPUs). Furthermore, the authors of \citet{Krizhevsky2012} provided the first publicly available implementation of a CNN running on a GPU by utilizing the Nvidia Compute Unified Device Architecture (CUDA) api. \citet{Krizhevsky2012} also demonstrated that their results are tied to the performance of the GPU in terms of available memory as well as the rate of floating point operations per second, with the implication that a better GPU can fit in memory and efficiently process larger datasets which would improve their results.

The convolutional layer can be described mathematically by utilizing the previously described principle of allowing the filter to only perceive a local neighborhood of the input. Consider the value of a single point $v_{i,j} \subset V \in{\mathbb{R}^2}$ where $i,j$ denote spatial indexes. Let $X \in{\mathbb{R}^3}$ be an input image of size $(D \times A \times B)$, and $W \in{\mathbb{R}^3}$ be a symmetric filter of size $(D \times r \times r)$ where $r$ is an odd number less than $A$ and $B$. Then, the value at a single point $v_{i,j}$ is given as follows,

\begin{equation}
    \label{eq:singlepointconv}
    v_{i,j} = \sum_{d=1}^D \sum_{a=1}^r \sum_{b=1}^r W_{a, b}^d X_{i+a-\lceil \frac{r}{2} \rceil,j+b-\lceil \frac{r}{2} \rceil}^d
\end{equation}

Where the subscript notation is used in $W$ and $X$ to denote indexes similar to $V$, and the superscript denotes the channel. Equation \ref{eq:singlepointconv} is described graphically in Figure \ref{fig:convlayer} for a single channel. Note that Equation \ref{eq:singlepointconv} is only valid given $(0 < i + a - \lceil \frac{r}{2} \rceil \leq A, 0 < i + b - \lceil \frac{r}{2} \rceil \leq B)$, which means that the filter can not be centered along a $\lfloor \frac{r}{2} \rfloor$ thick border of X. Hence Equation \ref{eq:singlepointconv} describes a valid convolution.

\begin{figure}
    \centering
    \includegraphics[width=.6\textwidth]{convlayer}
    \caption{\label{fig:convlayer}Convolution applied at a single point given a two dimensional input. Figure adapted from \protect\citet{Yamashita2018}.}
\end{figure}

Repeating Equation \ref{eq:singlepointconv} across all points $x \subset X$ by applying a sliding window technique where the filter is not centered along the previously defined border of X returns the convolution of X with filter W, which results in the output $V$ with size $(A-r+1) \times (B-r+1)$. The size of the output can be adjusted by padding the input $X$ by a size $P$ in each direction or increasing the stride $S$ of the sliding window, which reformulates the output size of $V$ as a function

\begin{equation}
    \label{eq:outputdim}
    (V_\text{dim1}, V_\text{dim2}) = \lfloor\frac{A - r + 2P}{S} + 1\rfloor, \lfloor\frac{B - r + 2P}{S} + 1\rfloor
\end{equation}

The convolutional layer adds the convolution described in equation (\ref{eq:singlepointconv}) with a bias term $\beta \in{\mathbb{R}^2}$ of the same spatial shape as $V$, as well as applying an activation function $g$ to each $v_{i,j}$ which introduces nonlinearity. In summary, the output of a convolutional layer can be described as 

\begin{equation}
    \label{eq:outputconv}
    V^\prime = g(V + \beta) = g(W^TX + \beta)
\end{equation}

If the number of filters increases from $1$ to $N$, Equation \ref{eq:outputconv} is repeated for all filters, resulting in an output $V \in{\mathbb{R}^3}$ of size $(V_\text{dim1}, V_\text{dim2}, N)$.

Finally, the receptive field refers to the number of neurons seen by a neuron deeper in the network, and can be seen in figure \ref{fig:convlayer} where the feature map pixel sees nine input tensor pixels. Following the derivations described in \citet{Araujo2019}, the receptive field at a layer is mathematically defined as

\begin{equation}
    \label{eq:receptivefield}
    r_0 = \sum_{l=1}^L\left(\left(k_l - 1\right)\prod_{i=1}^{l-1}s_i\right) + 1
\end{equation}

where $r_0$ is the receptive field at layer $L$, $k_l$ is the kernel size at layer $l$ and $s_i$ is the stride at layer $i$. Note that the stride in the last convolutional layer does not influence on the receptive field. The receptive field defined in equation \ref{eq:receptivefield} may be regarded as the theoretical upper bound, with recent results such as \citet{Luo2017} showing that the effective receptive field attains a Gaussian shape with a peak at the center of the receptive field. Hence the effective receptive field is smaller than the theoretical maximum, and assumes that pixels closer to the center of the receptive field are more important.

\subsection{Image segmentation}
\label{sec:image-segmentation}
Image segmentation is a computer vision task where pixels are assigned labels according to some predetermined rules. It is common to define an image segmentation task either as a study of countable \textit{things} (Instance segmentation), or recognizing similarly textured \textit{stuff} (Semantic segmentation) \citep{Kirillov2018}. The task for this thesis is to label forecasted sea ice concentration according to the ice categories defined by the World Meteorological Organization total concentration standard \citep{WMO2014}, which falls into the latter category following the definition of recognizing \textit{stuff} in \citet{Adelson2001}. I.e. the current task is to assign each pixel in a forecasted scene a single class label.

Network architectures based on the Convolutional Neural Network (CNN) e.g. \citep{LeCun1989,Ciresan2012,Krizhevsky2012,Simonyan2014,Szegedy2014,He2015a,Huang2016} can be used to perform pixelwise semantic segmentation, however the CNN architectures listed have been developed for image classification i.e. predicting a single label for the entire image. \citet{Ciresan2012a} presented an approach where a CNN (see the architecture of \citet{Ciresan2012}) was used to predict a label for all pixels in an image. Instead of processing the entire image at once, \citet{Ciresan2012a} applied a sliding window technique which predicted each pixel by using their surrounding neighborhood as input. However, due to only processing parts of the image at once, the segmentation algorithm in \citet{Ciresan2012a} is computationally expensive as the CNN must be run for all possible neighborhoods. Additionally, the context for each CNN is limited to the local neighborhood surrounding the pixel \citep{Ronneberger2015}.

To capture the global context of a scene, network architectures such as \citet{Long2015,Noh2015,Ronneberger2015,Badrinarayanan2017,Chen2018} implemented the Encoder-Decoder architecture, where the entire input scene is first processed by a CNN-like architecture referred to as the Encoder to produce a signal. The signal is then used as input to a subsequent network which reconstructs the encoded signal to match the resolution of the original image through upsampling. \citet{Long2015,Ronneberger2015,Badrinarayanan2017} all applied the deconvolution architecture proposed by \citet{Zeiler2010} to upsample the encoded signal through the use of a trainable deconvolutional layer, which will be described in greater detail in Section \ref{sec:tconv}. However, other upsampling techniques exist, such as unpooling used in \citet{Noh2015} which performs a upsampling by performing the opposite operation of a maxpool layer (maxpooling is described in Section \ref{sec:maxpool}).

This thesis will utilize the U-Net architecture proposed by \citet{Ronneberger2015}. The U-Net was initially developed for medical image segmentation, however the architecture has shown promising results for both Pan-Arctic seasonal \citep{Andersson2021} and regional short term \citep{Grigoryev2022} sea ice concentration forecasting amongst other applications. Another aspect which makes the U-Net more suitable to the current task, compared to other previously mentioned image-to-image architectures is that the network converges quickly, which is ideal when working with a dataset consisting of few samples \citep{Ronneberger2015}. A final benefit of utilizing the U-Net architecture is that the network learns to combine spatial features at different spatial scales \citep{Ronneberger2015} and (Equation \ref{eq:outputdim}). This improves the localization capabilities of the network and causes predictions to be more spatially precises, which is desireable for high-resolution output.

\subsection{Describing the U-Net architecture}
\label{sec:unet}
Figure \ref{fig:unet-overview} shows the U-Net architecture. This section intends to describe the different components constituting the architecture from a technical point of view. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{unet_screenshot}
    \caption{\label{fig:unet-overview}The U-Net architecture. The blue boxes represent feature maps, with the lower left numbers determining the spatial resolution and the top number the amount of feature maps. White boxes in the expansive path (right side / decoder) are the copied feature maps from the contractive path (left side / encoder). Arrows denote the different operations. Note that the original U-Net only performs \textit{valid} convolutions, i.e. convolution without padding to match the input. This causes a convolutional layer to slightly decrease the spatial extent. As a result, the copied features from the contracting path are also cropped to match the dimensionality in the expansive path. Figure extracted from \protect\citet{Ronneberger2015}.}
\end{figure}

\subsubsection{Convolutional layers}
The convolutional layers in the U-Net are structured in blocks. A single convolutional block consists of two repeat convolutional layers, each followed by the Rectified Linear Unit (ReLU) \citep{Nair2010} nonlinear activation function. The ReLU activation function is defined as follows

\begin{equation}
    f(x) = \max{(0,x)} \qquad
    \includegraphics[valign = c, width=.3\linewidth]{relu}
\end{equation}

The ReLU function, similar to other activation functions used in deep neural networks, introduce non-linearities to the connections in the network. Thus the network is able to learn non-linear connections in the data.

Each convolution is performed using a $3 \times 3$ window. The original formulation of the U-Net also does not apply padding to the input, resulting the convolutional filter only being applied to the entries of the input where the filter is never out of bounds. With a stride $S=1$, this results in each convolutional layer reducing the spatial extent by two pixels in each direction following Equation \ref{eq:outputdim}. It is also noted that the number of feature maps is doubled after each downsampling step, which is performed by the pooling layers.

\subsubsection{Maxpooling}
\label{sec:maxpool}
Pooling operations are used to reduce the spatial extent of the current feature maps, by downsampling the data in the spatial dimensions. As seen in Figure \ref{fig:unet-overview}, the U-Net downsamples the data in the contracting path through $2 \times 2$ maximum pool layers with a stride of 2. This specific configuration causes the spatial resolution to be halved. In the max-pool layer, a filter runs through each input channel and chooses the maximum value inside the neighborhood of the filter. As such, the extreme values in each feature map is retained at the expense of rejecting the rest of the data. Since the maxpooling operation is rejecting some parts of the data, it may be regarded as a regularizer for the network which aids in keeping the model generalized, which is a topic further explored in the final paragraph of Section \ref{sec:training-loop}. See Figure \ref{fig:maxpool} for a graphical description.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{The-MaxPool-operation}
    \caption{\label{fig:maxpool}The max-pool operation for a $2 \times 2$ filter with a stride of 2. Figure taken from \protect\citet{MihaiDaniel2020}}
\end{figure}

\subsubsection{Transposed convolutions}
\label{sec:tconv}
Transposed convolution was proposed by \citet{Zeiler2010} (note the incorrect use of deconvolution, this is not the mathematical inverse of a convolution) to increase the resolution of a feature map. The method was first utilized by \citet{Long2015} to connect the coarse output of an encoder with the image resolution of the target (it is referred to as both \textit{backwards convolution} and \textit{deconvolution} in the proceedings paper). Similar to the convolutional layer, the transposed convolutional layer involves striding a convolutional filter with trainable parameters across a feature map. However, the transposed convolutional layer projects a singular entry from the input through the convolutional kernel to produce an output that is larger than the input. Figure (\ref{fig:tconv}) shows a graphical description of transposed convolutions.

\begin{figure}
    \centering
    \includegraphics[trim = {0 5.125cm 0 0}, clip, width = \textwidth]{applsci-12-12075-g004}
    \caption{\label{fig:tconv}Figure demonstrating the computations performed by a transposed convolutional layer. Figure adapted from \protect\citet{Wu2022}}
\end{figure}

\subsubsection{Expansive path and skip-connections}
In the encoder architecture, lower level feature maps provide spatial information regarding where stuff is located in a scene, whereas higher level feature maps contain information regarding what is in the scene at the expense of losing spatial information \citep{Long2015}. \citet{Ronneberger2015} utilize skip-connections to circumvent that the output is only based on the fully encoded signal, as well as to learn information at different spatial scales from the different levels of the encoder. The skip-connections concatenate the features from the contracting path with the output from the transposed convolution at the same level of depth (Figure \ref{fig:unet-overview} gray arrows). To clarify, the contracting and expanding path is at the same level when the number of feature maps are equal at the end of the convolutional block. 

The concatenation operation is possible in \citet{Ronneberger2015} since a crop operation is applied to the feature maps in the encoder such that the spatial dimensions in the encoder matches the spatial dimensionality of the feature maps in the decoder. The operation can be seen in Figure (\ref{fig:unet-overview}) denoted by the gray arrow. The resulting convolutional layer is then trained to make a more precise prediction due to the concatenated input \citep{Ronneberger2015}. It is noted for future reference that the crop operation is not necessary if a convolution operation which preserves the dimensionality of the input feature map through the use of padding is applied (Equation \ref{eq:outputdim}).

\subsubsection{Outputs}
The output layer of the U-Net is denoted by the turquoise arrow at the right side of Figure \ref{fig:unet-overview}. The arrow denotes that the input is processed by a convolutional layer which has as many filters as there are output classes. Each filter is of size $(1 \times 1)$ with stride $S=1$ and maps each layer in the input feature map to their respective class probability map of equal spatial shape \citep{Ronneberger2015}. By inspecting Figure \ref{fig:unet-overview}, the U-Net outputs two feature maps, and from each feature map the pixelwise probability of belonging to the associated class can be computed.

\subsection{Training procedure for the U-Net}
\label{sec:training-loop}
This subsection aims to demonstrate how \citet{Ronneberger2015} trained the U-Net, and will consequently highlight some different hyperparameters and exemplify some functions and operations which are used in the training. Hyperparameters refer to model parameters which are not updated during training \citep{Yu2020a}, and may directly influence the model architecture or the training procedure. This section will not describe how samples were preprocessed and loaded, and modifications made which reflects concerns regarding medical images may be noted but not explained.

Training the U-Net starts by assigning random values to the weights of the network. Since the U-Net utilizes the ReLU activation function after each convolutional layer in the convolutional blocks \citep{Ronneberger2015}, it is standard for each layer to draw the weights from a normal distribution with $\mu = 0$ and standard deviation $\sigma = \sqrt{\frac{2}{n_l}}$, where $n_l$ is the number of inputs to the layer \citep{He2015}. This weight initialization scheme ensures that variance of the feature maps are approximately equal, i.e. avoids varying the activation of input signals between layers \citep{He2015,Ronneberger2015}.

The process of training the U-Net involves making predictions on all training data. For each sample, the prediction is compared against a ground truth label. For the U-Net, a pixelwise prediction map is created by computing the pixelwise softmax which is an extension of the softmax function \citep{Bridle1990} defined as 

\begin{equation}
    \label{eq:psoftmax}
    p_{k,i,j} = \frac{e^{a_{k,i,j}}}{\sum_{k^{\prime} = 1}^Ke^{a_{k^\prime,i,j}}}
\end{equation}

where $a_k$ is the feature map for feature channel $k$ of input $x$ and $K$ is the number of output classes and $p \in \left[0, 1\right]$. $i,j$ are the spatial coordinates. Similarly to the standard softmax function \citep{Bridle1990}, Equation \ref{eq:psoftmax} is approximately 1 for the class that has maximum $a_{k,i,j}$ and close to 0 for all other classes, albeit depthwise in the channel dimension for all pixels \citep{Ronneberger2015}. The sum of the depthwise output from the pixelwise softmax is 1, hence the function maps each pixel with the probability of that pixel belonging to each class. 

For the case of binary classification, i.e. when the number of classes $k=2$, the softmax function in equation (\ref{eq:psoftmax}) is reduced to the Sigmoid function which is defined as, 

\begin{equation}
    \label{eq:sigmoid}
    p_{i,j} = \frac{e^{a_{i,j}}}{e^{a_{i,j}} + 1} \qquad
    \includegraphics[valign = c, width=.3\linewidth]{sigmoid}
\end{equation}

To quantify the prediction error, a loss function is defined. The overall goal of training a neural network is to minimize the loss function with respect to the trainable weights. For the U-Net, a weighted variation of the cross entropy loss function is proposed \citep{Ronneberger2015}. 

\begin{equation}
    \label{eq:unet-loss}
    L(p) = \sum_{i,j \subset \mathbb{Z}}w_{i,j}log(p_{l,i,j})
\end{equation}

where $w$ is a predefined weight map and $p_{l,i,j}$ (equation \ref{eq:psoftmax}) is the prediction made at pixel $i,j$ at the true label $l$.

The error computed by the loss function is then sent backwards throughout the network according to the backpropagation algorithm \citep{Rumelhart1986}, which effectively computes the gradient of the loss function with regards to the trainable parameters 
\begin{equation}
    \label{eq:weight_adjust}
    \frac{\partial L}{\partial w_l} = \frac{\partial L}{\partial p}\frac{\partial p}{\partial w_l}
\end{equation}
where $w_l$ is the trainable parameters associated with the $l$-th layer. The gradient of the loss for a weight at a given layer shown in Equation \ref{eq:weight_adjust} is used by an optimizer to adjust the weights such that the loss is minimized with respect to the weights (gradient descent).

\citet{Ronneberger2015} uses the stochastic gradient descent with momentum optimizer implemented in the Machine Learning library Caffe \citep{Jia2014}, where the optimizer is defined as follows,

\begin{equation}
    \label{eq:msgd}
    w_l^{t+1} = \gamma(w_l^t - w_l^{t-1}) - \mu\frac{\partial L}{\partial w_l^t}
\end{equation}

In Equation \ref{eq:msgd}, the superscript $t$ was added to $w$ and refers to training step, which is defined as a prediction and subsequent backpropagation of a batch of samples, where the size of a batch is a pre-determined hyperparameter. $\gamma$ and $\mu$ are the momentum and learning rate hyperparameters respectively. Note that $\gamma$ is introduced by momentum stochastic gradient descent, whereas the learning rate $\mu$ is a hyperparameter common for all deep learning models and determines the rate of weight adjustment as seen in equation (\ref{eq:msgd}).

When all training samples have been inspected once by the U-Net, the training data is shuffled and the above outlined training procedure is repeated. The process of going through all the training data once is defined as an epoch. The number of epochs is a hyperparameter which can be adjusted, and is tied to the bias-variance tradeoff dilemma \citep{Geman1992}. Moreover, the number of epochs determines the duration of training time, and is influenced by the available computing resources.

\citet{Geman1992} states that the cost of low bias in a model is high variance. A model with high bias and low variance is assumed to not have underwent much, if any training, and is thus underfitted to the data. Consequently, a model with low bias but high variance has been trained for a high number of epochs, and is overfitted towards the training-data. An overfitted model is, due to its high variance, ideal at explaining the training data, but lacks the ability to generalize to external datasets. For the training procedure described above, the optimum model has been trained for a sufficient amount of epochs, where it is neither underfitted nor overfitted.

\subsection{Forecast verification metrics}
Verification schemes provide insight into how a forecasting system performs. For this thesis, verification metrics serve a dual purpose. From a model development point of view, verification metrics will be used to increase the skill of the model. However, the same metrics will also be utilized to assess the quality of a prediction as well as explaining the physical interpretation of the model \citep{Casati2008}. The model developed for this thesis predicts a scene consisting of labelled pixels, as described in Section \ref{sec:image-segmentation}. It was mentioned in Section \ref{sec:introduction} that the developed model is aimed towards operational end users, which is partly achieved by validating the model against metrics of end user relevance. Furthermore, it can be assumed that the model and target observations will not differ much outside of the Marginal Ice Zone (MIZ) \citep{Fritzner2020}. Thus, this section will introduce metrics which are relevant for evaluating the sea-ice edge position, as the sea ice edge is important information for maritime operators in the Arctic \citep{Melsom2019}. The following subsections will describe how to determine the position of the sea ice edge, as well as its length according to \citet{Melsom2019}, and derive the Integrated Ice Edge Error \citep{Goessling2016}, with regards to a spatially gridded dataset of deterministic sea ice concentration values.

The Integrated Ice Edge Error is chosen among similar sea ice edge metrics \citep{Melsom2019, Dukhovskoy2015} as it has been shown to be less sensitive to isolated ice patches \citep{Palerme2019}. Furthermore, the work of \citet{Melsom2019} recommends the Integrated Ice Edge Error amongst other metrics for its intuitive interpretation as well as for the possibility to provide the spatial distribution of IIEE areas.

\subsubsection{Defining the Sea Ice Edge}
\label{sec:iceedgelength}
The sea ice edge for a given spatial distribution of sea ice concentration values is derived on a per pixel basis. Let $C \in{\mathbb{R}^2}$ be gridded sea ice concentration values. Then, the sea ice edge is defined as the entries in $C$ which meets the following condition,

\begin{equation}
    \label{eq:iceedge}
    c_{i,j} \geq c_e \wedge \text{min}{(c_{i-1,j},c_{i+1,j},c_{i,j-1},c_{i,j+1})} < c_e
\end{equation}

In Condition (\ref{eq:iceedge}), $c \subset C$ are sea ice concentration values, with $i,j$ denoting indexes. $c_e$ is a given concentration threshold.

Next, let $E \in{\mathbb{R}^2}$ be the set containing sea ice concentration pixels constituting the sea ice edge. It can be seen that the entries $c_{i,j}$ which adhere to condition (\ref{eq:iceedge}) form the set $E$ \citep{Melsom2019}.

Moreover, all the entries in $E$ each contribute to the total length of the sea ice edge, with each entries' length contribution determined based on that entries' 4-connected adjacent grid points, (see figure \ref{fig:4-connection}). Using this formulation, the different combination of neighborhoods in $E$ can result in three different length contributions. For the following contributions, $s$ is the spatial resolution of the grid.

\begin{figure}
    \centering
    \includestandalone[width=.3\textwidth]{thesis/methodology/figures/4-connection}
    \caption{\label{fig:4-connection}The gray pixels forms the 4-connected neighborhood of adjacent grid cells for the center pixel.}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = .5\textwidth]{MelsomIEL.pdf}
    \caption{\label{fig:iceedgeschematic}Sketch of an example gridded ice edge. The gray cells denote ice edge cells, which are labelled and illustrates the ice edge contained in the cell. The black cells denote land. Figure fetched from \protect\citet{Melsom2019}}
\end{figure}

\begin{itemize}
    \item A neighborless pixel is assumed to yield a contribution equal to the length of the diagonal of a grid cell ($l = \sqrt2s$). Here it is assumed that the grid cell only have diagonal neighbors ($\text{e}_\text{a}$ in figure \ref{fig:iceedgeschematic}).
    \item A pixel with one of the four possible adjacent grid points contributes with the mean value between the length of the grid cell and length og the diagonal of the grid cell $l = \frac{s + \sqrt2s}{2}$. It is assumed that the grid cell also has a diagonal neighbor ($\text{e}_\text{b}$ and $\text{e}_\text{e}$ in figure \ref{fig:iceedgeschematic}).
    \item A pixel with two or more of the four adjacent grid points contributes with its spatial resolution (length of the grid cell) $l = s$ ($\text{e}_\text{c}$ and $\text{e}_\text{d}$ in figure \ref{fig:iceedgeschematic}).
\end{itemize}

The final length of the sea ice edge length then becomes

\begin{equation}
    \label{eq:ice-edge-length}
    L = \sum_\text{e in E} l^e
\end{equation}

where the superscript $l^e$ denotes the length associated with the entry $e$ according to the algorithm listed above, I.e. the sum of all contributions.

\subsubsection{Integrated Ice Edge Error}
\label{sec:iiee}
The IIEE is an error metric which compares a forecast $f$ to a predefined ground truth target $t$ \citet{Goessling2016}. The metric is defined as

\begin{equation}
    \label{eq:IIEE}
    \text{IIEE} = \text{O} + \text{U}
\end{equation}

where 

\begin{equation}
    \label{eq:a_plus}
    \text{O} = \int_A\text{max}(C_f - C_t, 0)dA
\end{equation}

and

\begin{equation}
    \label{eq:a_minus}
    \text{U} = \int_A\text{max}(C_t - C_f, 0)dA
\end{equation}

with $A \in{\mathbb{R}^2}$ being the area of interest, and is of similar size as $C$. Subscript $f,t$ denotes whether $C$ contains forecasted or target sea ice concentration values. In Equations \ref{eq:a_plus} and \ref{eq:a_minus}, C is binary and is equal to 1 if its concentration value is above a predefined threshold, and 0 elsewhere \citep{Goessling2016}. From the definition of the metric, it can be seen that the IIEE is a sum of the forecast overshoot and undershoot compared to the ground truth target. For a graphical description, see Figure \ref{fig:goessling_iiee}.

\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{goessling_iiee}
    \caption{\label{fig:goessling_iiee}15\% sea ice concentration contours for a forecast (blue) and target (red) sea ice concentration product. The IIEE is the sum of the overestimated (O, blue) and underestimated (U, red). White denotes the union between the products. Figure fetched from \protect\citet{Goessling2016}.}
\end{figure}

Additionally, the IIEE can also be represented as a spatial metric by removing the integral with respect to $A$ in Equation \ref{eq:a_plus} and \ref{eq:a_minus}. In this way, the metric is used to define the set of pixels which constitutes its area. To clearly distinguish between the area O (overestimation) and the set of pixels used to compute O, $A^+$ will be used to note the latter. Similarly, $A^-$ will represent the set of pixels constituting U (underestimation). Finally, it can be seen that $A^+$ and $A^-$ represent the spatial distribution of False Positive and False Negatives of the forecast respectively.

The length of the ice edge has a strong influence on the IIEE \citep{Goessling2018,Palerme2019}. Hence, to ensure that forecast errors are comparable across seasons, IIEE is normalized with the length of the ice edge, as mentioned in section (\ref{sec:osisaf}). Furthermore, the normalized IIEE (NIIEE) provides an estimate of the displacement error between the forecasted and target sea ice edge \citep{Melsom2019}.

\subsection{AI explainability}
Explainable Artificial Intelligence (XAI) is a field which has seen a recent relevance growth in conjunction with the renewed interest in deep learning methods (especially for image analysis) launched by the network proposed by \citet{Krizhevsky2012}. XAI covers methods which aim to provide insight into the "black-box problem" of machine learning \citep{Adadi2018}, which for deep learning models arise partly due to the complex structures and many nonlinear connections found in the models \citep{Lopes2022}. For the purpose of this thesis, XAI methods which aim explaining a single decision will be applied both to increase the transparency of the developed deep learning system, as well as attempting to connect a single prediction to the underlying physics present in the input variables.

\subsubsection{Gradient-weighted Class Activation Mapping for semantic segmentation}
\label{sec:seg-grad-cam}
Several methods of XAI for visual explanations for decisions made by CNN-based models exist e.g. \citep{Simonyan2013,Zhou2016,Selvaraju2016,Sundararajan2017,Lundberg2017}. The aforementioned methods are designed for image classification tasks, which covers problems where the entire image is associated with a single label. However, there has been limited work related to understanding the decisions made by semantic segmentation models \citep{Linardatos2020}.

A method for activation based explanation of semantic segmentation predictions was proposed by \citet{Vinogradova2020}, which is a modification of the Gradient-weighted Class Activation Map (Grad-CAM) that was first introduced by \citet{Selvaraju2016}. Grad-CAM is an activation based explanation method, hence, Grad-CAM constructs a class activated map that highlights a weighted combination of all feature maps at a given layer for a given class. The assigned weights determines whether each feature map was considered important when predicting the considered class, and are computed from the gradient of the predicted logit with respect to each feature map at the considered layer \citep{Selvaraju2016}. Grad-CAM is mathematically defined as

\begin{equation}
    \label{eq:gradcam}
    L^c = \text{ReLU}\left(\sum_k \alpha_k^c A^k\right)    
\end{equation}

with the weights computed as

\begin{equation}
    \label{eq:importance-weight}
    \alpha_k^c = \frac{1}{Z} \sum_{i,j} \frac{\partial{y^c}}{\partial{A_{ij}^k}}
\end{equation}

where $c$ denotes the chosen class of interest, $k$ the number of feature maps and $i,j$ the spatial dimensions. $A \in \mathbb{R}^3$ are $k$ feature maps of size $i,j$ and $y^c$ is the predicted logit for class $c$. Finally $Z$ denotes the number of nodes in feature map $A$. 

Figure \ref{fig:gradcam} provides an overview which summarizes Equations \ref{eq:gradcam} and \ref{eq:importance-weight}. The figure also demonstrates the use of the deepest layer as the chosen feature maps, which \citet{Selvaraju2016} argues for as they contain object specific information rather than positional information, although the choice of feature map is a free parameter. Moreover, the output class activation map to the lower left corner of figure (\ref{fig:gradcam}) represents where the model had to look to make the prediction \citep{Selvaraju2016}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{GradCAM}
    \caption{\label{fig:gradcam}Overview showing how a class activation map is constructed form a single image. The bottom left image (captioned "Grad-CAM") provides an example class activation map for the class "tiger cat". The figure is heavily adapted from \protect\citet{Selvaraju2016}, where references in the figure to Guided Grad-CAM and Guided backpropagation has been removed.}
\end{figure}

Furthermore, the modification presented by \citet{Vinogradova2020}, hereafter referred to as seg-Grad-CAM, replaces $y^c$ in Equation \ref{eq:importance-weight} with a new term 

\begin{equation}
    \sum_{(i,j) \in M} y_{ij}^c
\end{equation}

where $M$ is a set of pixel indices \citep{Vinogradova2020}. Thus, the pixel indices chosen become a free parameter. Similarly to \citet{Selvaraju2016}, \citet{Vinogradova2020} also argues that computing the gradient w.r.t. the lowest resolution feature maps returns the most informative class activation maps. Hence, contrary to Grad-CAM which considers encoder only networks where the final convolutional layer contain the lowest resolution feature maps, seg-Grad-CAM compute the gradient w.r.t. the second convolutional layer located in the bottleneck of the U-Net, which in Figure \ref{fig:unet-overview} is represented by the lower right blue rectangle (followed by the first up-conv arrow).

\biblio

\end{document}
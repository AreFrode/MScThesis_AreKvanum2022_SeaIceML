\documentclass[../main/thesis.tex]{subfiles}
\graphicspath{{/home/arefk/uio/MScThesis_AreKvanum2022_SeaIceML/thesis/methodology/figures/}}


\begin{document}
\section{Methodological framework}
This section will first outline the theoretical background of convolutions from a deep learning point of view, as well as provide a brief overview of image segmentation as a computer vision task. Second, the methodological framework of the U-Net architecture, the deep neural network which is used in the present work, is outlined with a detailed description of its training loop and central algorithms. Finally, validation metrics which are used to asses the performance of the developed deep learning system will be described.

\subsection{Convolutional layers}
\label{sec:convolutional-layer}
Convolutional layers incorporated into a deep neural network which are utilizing the backpropagation algorithm \citep{Rumelhart1986} was initially proposed by \citet{LeCun1989} to classify handwritten numbers. The layer \citet{LeCun1989} presented consists of an arbitrary amount of filters, which are small two dimensional matrices (e.g. $(3 \times 3)$ pixels) designed to capture a certain structure in the image such as lines or edges \todo{Sitere boka til Goodfellow2016?}. Each filter contains trainable weights, which are learned from the data during backpropagation \citep{LeCun1989} and gradient descent. When a filter is convolved with all possible local neighborhoods from the input, it outputs a feature map which represent where the input image triggered a response from the filter \citep{Zeiler2010}. Moreover, inputting feature maps to a convolutional layer allows for the filters to respond to combinations of lower level structures, which trains the layer to detect more complicated patterns \citep{Fukushima1980}. Additionally, stacking convolutional layers in a network-architecture structure increases the field of view for each subsequent layer, which makes each layer observe an increasingly complex pattern of higher order feature maps at increasingly larger spatial scales \citep{Fukushima1980}. As a result, convolutional layers are able to discern between object and background as they perceive only a limited view of the scene. The convolutional layer is also invariant to the translation of the object, since the filter is constant when creating the feature map, i.e. the filter is detecting the same feature at all locations in the image, known as weight sharing \citep{LeCun1989}.

The number of trainable parameters for a convolutional layer is equal to the size of a filter times the number of filters. As a result, the number of trainable parameters is invariant to the spatial extent of the input images. Contrarily, fitting a fully connected layer to spatial gridded data consists of associating a separate trainable parameter to each pixel. As such, the size of a fully connected layer scales with the size of the image, which increases the risk of overfitting the network. In the case of the convolutional layer, \citet{LeCun1989} notes that reducing the number of trainable parameters through weight sharing constrains the solution space such that overfitting is avoided while still having enough trainable parameters to fit the layer to the data. Furthermore, the fully connected layer is not invariant to translation as each trainable parameter is exclusive to their respective pixel, hence no weight sharing. As such, the layer is unable to detect a similar object at a different position, reducing their usefulness for image-based prediction tasks.

Finally, \citet{Ciresan2012} showed that the processing time of a convolutional layer is significantly shortened by utilizing a graphics processing unit (GPU), due to their large amount of compute cores compared to traditional Central Processing Units (CPUs). Furthermore, the authors of \citet{Krizhevsky2012} provided the first publicly available implementation of a CNN running on a GPU by utilizing the Nvidia Compute Unified Device Architecture (CUDA) api. \citet{Krizhevsky2012} also demonstrated that their results are tied to the performance of the GPU in terms of available memory as well as the rate of floating point operations per second, with the implication that a better GPU as well as larger datasets would improve their results. As such, both larger convolutional layers as well as deeper convolutional architectures written in a machine learning library which interacts with the GPU through CUDA \citep{Jia2014,tensorflow2015-whitepaper,Paszke2019} can be initiated. This is due to the speed up induced by the GPU, which allows for larger datasets to be processed, thus satisfying the increased parameter-count of the architecture.

since they can process greater datasets consisting of larger samples due to their GPU implementation.

The convolutional layer can be described mathematically by utilizing the previously described principle of allowing the filter to only perceive a local neighborhood of the input. Consider the value of a single point $y_{i,j} \subset Y \in{\mathbb{R}^2}$ where $i,j$ denote the position in the x and y direction as a single output from a convolution. Let $X \in{\mathbb{R}^3}$ be an input image of size $(A \times B \times D)$ consisting of a single channel, and $W \in{\mathbb{R}^3}$ be a symmetric filter of size $(r \times r \times D)$. Then, the value at a single point $y_{i,j}$ is given as follows,

\begin{equation}
    \label{eq:singlepointconv}
    y_{i,j} = \sum_{a=1-\frac{r}{2}}^\frac{r}{2} \sum_{b=1-\frac{r}{2}}^\frac{r}{2} \sum_{d=1}^D W_{a + \frac{r}{2},b + \frac{r}{2}, d} X_{i+a,j+b,d}
\end{equation}

Where the subscript notation is used in $W$ and $X$ to denote indexes similar to $Y$. Equation (\ref{eq:singlepointconv}) is described graphically in figure (\ref{fig:convlayer})

\begin{figure}
    \centering
    \includegraphics[width=.6\textwidth]{convlayer}
    \caption{\label{fig:convlayer}Convolution applied at a single point given a two dimensional input. Figure adapted from \protect\citep{Yamashita2018}.}
\end{figure}

Repeating equation (\ref{eq:singlepointconv}) across all points $x \subset X$ by applying a sliding window technique returns the convolution of X with filter W, which results in an output $Y$ with size $(A-r+1) \times (B-r+1)$. Note that the above definition only applies for $X_{1 \leq i+a \leq A, 1 \leq j+b \leq B}$. The size of the output can be adjusted by padding the input $X$ by a size $P$ in each direction or increasing the stride $S$ of the sliding window, which reformulates the output size of $Y$ in a single dimension as a function

\begin{equation}
    \label{eq:outputdim}
    Y_\text{dim} = \lfloor\frac{A - r + 2P}{S} + 1\rfloor    
\end{equation}

The convolutional layer adds the convolution described in equation (\ref{eq:singlepointconv}) with a bias term $B \in{\mathbb{R}^2}$ of the same spatial shape as $Y$, as well as applying an activation function $g$ to each $y_{i,j}$ which introduces nonlinearity. In summary, the output of a convolutional layer can be described as 

\begin{equation}
    \label{eq:outputconv}
    Y^\prime = g(Y + B) = g(W^TX + B)
\end{equation}

If the number of filters increases from $1$ to $N$, equation (\ref{eq:outputconv}) is repeated for all filters, resulting in an output $Y \in{\mathbb{R}^3}$ of size $(Y_\text{dim1}, Y_\text{dim2}, N)$.

\subsection{Image segmentation}
\label{sec:image-segmentation}
Image segmentation is a computer vision task where pixels are assigned labels according to some predetermined rules. It is common to define an image segmentation task either as a study of countable \textit{things} (Instance segmentation), or recognizing similarly textured \textit{stuff} (Semantic segmentation) \citep{Kirillov2018}. The task for this thesis, which is labeling sea ice concentration according to its predicted concentration class, falls into the latter category following the definition of \textit{stuff} in \citet{Adelson2001}. I.e. the current task is to assign each pixel in a predicted scene a single class label.

Network architectures based on the Convolutional Neural Network (CNN) \citep{LeCun1989,Ciresan2012,Krizhevsky2012,Simonyan2014,Szegedy2014,He2015a,Huang2016} can be used to perform pixelwise semantic segmentation, however the CNN architectures listed have been developed for image classification i.e. predicting a single label for the entire image. \citet{Ciresan2012a} presented an approach where a CNN (see the architecture of \citet{Ciresan2012}) was used to predict a label for all pixels in an image. Instead of processing the entire image at once, \citet{Ciresan2012a} applied a sliding window technique which predicted each pixel by using their surrounding neighborhood as input. However, due to only processing parts of the image at once, the segmentation algorithm in \citet{Ciresan2012a} is computationally expensive as the CNN must be run for all possible neighborhoods. Additionally, the context for each CNN is limited to the local neighborhood surrounding the pixel \citep{Ronneberger2015}.

To capture the global context of a scene, network architectures such as \citet{Long2015,Noh2015,Ronneberger2015,Badrinarayanan2017,Chen2018} implement the Encoder-Decoder architecture, where the entire input scene is first processed by a CNN-like architecture referred to as the Encoder to produce a signal. The signal is then used as input to a subsequent network which reconstructs the encoded signal to match the resolution of the original image through upsampling. \citet{Long2015,Ronneberger2015,Badrinarayanan2017} all apply the Deconvolution architecture proposed by \citet{Zeiler2010} to upsample the encoded signal through the use of a trainable deconvolutional layer, which will be described in greater detail in Subsection (\ref{sec:tconv}). However, other upsampling techniques exists, such as unpooling used in \citet{Noh2015} which performs a upsampling by performing the opposite operation of a maxpool layer (maxpooling is described in Subsection (\ref{sec:maxpool})).

This thesis will utilize the U-Net architecture proposed by \citet{Ronneberger2015}. The U-Net was initially developed for medical image segmentation, however the architecture has shown promising results for both pan-arctic seasonal \citep{Andersson2021} and regional short term \citep{Grigoryev2022} sea ice concentration forecasting amongst other applications. Another aspect which makes the U-Net more suitable to the current task, compared to other previously mentioned image-to-image architectures is that the network converges quickly, which is ideal when working with a dataset consisting of few samples \citep{Ronneberger2015}.

\subsection{Describing the U-Net architecture}
Figure (\ref{fig:unet-overview}) shows the U-Net architecture. This section intends to describe the different components constituting the architecture from a technical point of view. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{unet_screenshot}
    \caption{\label{fig:unet-overview}The U-Net architecture. The blue boxes represent feature maps, with the lower left numbers determining the spatial resolution and the top number the amount of feature maps. White boxes in the expansive path (right side / decoder) are the copied feature maps from the contractive path (left side / encoder). Arrows denote the different operations. Note that the original U-Net only performs \textit{valid} convolutions, i.e. convolution without padding to match the input. This causes a convolutional layer to slightly decrease the spatial extent. As a result, the copied features from the contracting path are also cropped to match the dimensionality in the expansive path. Figure extracted from \protect\citet{Ronneberger2015}.}
\end{figure}

\subsubsection{The convolutional block}
A single convolutional block consists of two repeat convolutional layers, each followed by the Rectified Linear Unit (ReLU) \citep{Nair2010} nonlinear activation function. The ReLU activation function is defined as follows

\begin{equation}
    f(x) = \max{(0,x)} \qquad
    \includegraphics[valign = c, width=.3\linewidth]{relu}
\end{equation}

The ReLU function, similar to other activation functions used in deep neural networks, introduce non-linearities to the connections in the network. Thus the network is able to learn non-linear connections in the data.

Each convolution is performed using a $3 \times 3$ window. The original formulation of the U-Net also does not apply padding to the input, resulting the convolutional filter only being applied to the entries of the input where the filter is never out of bounds. With a stride $S=1$, this results in each convolutional layer reducing the spatial extent by two pixels in each direction following equation (\ref{eq:outputdim}). It is also noted that the number of feature maps is doubled after each downsampling step, which is performed by the pooling layers.

\subsubsection{Maxpooling}
\label{sec:maxpool}
Pooling operations are used to reduce the spatial extent of the current feature maps, by downsampling the data in the spatial dimensions. As seen in Figure (\ref{fig:unet-overview}), the U-Net downsamples the data in the contracting path through $2 \times 2$ maximum pool layers with a stride of 2. This specific configuration causes the spatial resolution to be halved. In the max-pool layer, a filter runs through each input channel and and chooses the maximum value inside the neighborhood of the filter. As such, the extreme values in each feature map is retained at the expense of rejecting the rest of the data. Since the maxpooling operation is rejecting some parts of the data, it may be regarded as a regularizer for the network which aid in keeping the model generalized, which is a topic further explored in the final paragraph of section \ref{sec:training-loop}. See Figure (\ref{fig:maxpool}) for a graphical description.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{The-MaxPool-operation}
    \caption{\label{fig:maxpool}The max-pool operation for a $2 \times 2$ filter with a stride of 2. Figure taken from \protect\citep{MihaiDaniel2020}}
\end{figure}

\subsubsection{Transposed convolutions}
\label{sec:tconv}
Transposed convolution was proposed by \citet{Zeiler2010} (note the incorrect use of deconvolution, this is not the mathematical inverse of a convolution) to increase the resolution of a feature map. The method was first utilized by \citet{Long2015} to connect the coarse output of an encoder with the image resolution of the target (it is referred to as both \textit{backwards convolution} and \textit{deconvolution} in the proceedings paper). Similar to the convolutional layer, the transposed convolutional layer involves striding a convolutional filter with trainable parameters across a feature map. However, the transposed convolutional layer projects a singular entry from the input through the convolutional kernel to produce an output that is larger than the input. Figure (\ref{fig:tconv}) shows a graphical description of transposed convolutions.

\begin{figure}
    \centering
    \includegraphics[trim = {0 5.125cm 0 0}, clip, width = \textwidth]{applsci-12-12075-g004}
    \caption{\label{fig:tconv}Figure demonstrating the computations performed by a transposed convolutional layer. Figure adapted from \protect\citep{Wu2022}}
\end{figure}

In the Encoder architecture, lower level feature maps provide spatial information regarding where stuff is located in a scene, whereas higher level feature maps contain information regarding what is in the scene at the expense of losing spatial information \citep{Long2015}. To circumvent this, \citet{Ronneberger2015} concatenate the features from the contracting path with the output from the transposed convolution at the same level of depth, i.e. where the number of feature maps are equal at the end of the convolutional block. The concatenation operation is possible in \citet{Ronneberger2015} since they crop the feature maps in the encoder in their spatial dimensions to match the spatial dimensionality of the feature maps in the decoder. The operation can be seen in Figure(\ref{fig:unet-overview}) denoted by the gray arrow. The resulting convolutional layer is then trained to make a more precise prediction due to the concatenated input \citep{Ronneberger2015}.

\subsubsection{Outputs}
The output layer of the U-Net is denoted by the turquoise arrow at the right side of Figure (\ref{fig:unet-overview}). The arrow denote that the input is processed by a convolutional layer which have as many filters as there are output classes. Each filter is of size $(1 \times 1)$ with stride $S=1$ and maps each layer in the input feature map to their respective class probability map of equal spatial shape \citep{Ronneberger2015}. By inspecting Figure (\ref{fig:unet-overview}), the U-Net outputs two feature maps, and from each feature map the pixelwise probability of belonging to the associated class can be computed.

\subsection{Training procedure for the U-Net}
\label{sec:training-loop}
This subsection aims to demonstrate how \citet{Ronneberger2015} trained the U-Net, and will consequently highlight some different hyperparameters and exemplify some functions and operations which are used in the training. Hyperparameters refer to model parameters which are not updated during training \citep{Yu2020a}, and may directly influence the model architecture or the training procedure. This section will not describe how samples were preprocessed and loaded, and modifications made which reflects concerns regarding medical images may be noted but not explained.

Training the U-Net starts by assigning random values to the weights of the network. Since the U-Net utilizes the ReLU activation function after each convolutional layer in the convolutional blocks \citep{Ronneberger2015}, it is standard for each layer to draw the weights from a normal distribution with $\mu = 0$ and standard deviation $\sigma = \sqrt{\frac{2}{n_l}}$, where $n_l$ is the number of inputs to the layer \citet{He2015}. This weight initialization scheme ensures that variance of the feature maps are approximately equal, i.e. avoids varying the activation of input signals between layers \citet{He2015,Ronneberger2015}.

The process of training the U-Net involves making predictions on all training data. For each sample, the prediction is compared against a ground truth label. For the U-Net, a pixelwise prediction map is created by computing the pixelwise softmax which is an extension of the softmax function \citep{Bridle1990} defined as 

\begin{equation}
    \label{eq:psoftmax}
    p_{k,i,j} = \frac{e^{a_{k,i,j}}}{\sum_{k^{\prime} = 1}^Ke^{a_{k^\prime,i,j}}}
\end{equation}

where $a_k$ is the feature map for feature channel $k$ of input $x$ and $K$ is the number of output classes and $p \in \left[0, 1\right]$. $i,j$ are the spatial coordinates. Similarly to the standard softmax function \citep{Bridle1990}, equation (\ref{eq:psoftmax}) is $\approx 1$ for the class that has maximum $a_{k,i,j}$ and $\approx 0$ for all other classes, albeit depthwise in the channel dimension for all pixels \citep{Ronneberger2015}. The sum of the depthwise output from the pixelwise softmax is 1, hence the function maps each pixel with the probability of that pixel belonging to each class. 

For the case of binary classification, i.e. when the number of classes $k=2$, the softmax function in equation (\ref{eq:psoftmax}) is reduced to the Sigmoid function which is defined as, 

\begin{equation}
    p_{i,j} = \frac{e^{a_{i,j}}}{e^{a_{i,j}} + 1} \qquad
    \includegraphics[valign = c, width=.3\linewidth]{sigmoid}
\end{equation}

To quantify the prediction error, a loss function is defined. The overall goal of training a neural network is to minimize the loss function with respect to the trainable weights. For the U-Net, a weighted variation of the cross entropy loss function is proposed \citep{Ronneberger2015}. 

\begin{equation}
    L(p) = \sum_{i,j \subset \mathbb{Z}^2}w_{i,j}log(p_{l,i,j})
\end{equation}

where $w$ is a predefined weight map and $p_{l,i,j}$ is the prediction made at pixel $i,j$ at the true label $l$.

The error computed by the loss function is then sent backwards throughout the network according to the backpropagation algorithm \citep{Rumelhart1986}, which effectively computes the gradient of the loss function with regards to the trainable parameters 
\begin{equation}
    \label{eq:weight_adjust}
    \frac{\partial L}{\partial w_l} = \frac{\partial L}{\partial p}\frac{\partial p}{\partial w_l}
\end{equation}
where $w_l$ is the trainable parameters associated with the $l$-th layer. The gradient of the loss for a weight at a given layer shown in equation (\ref{eq:weight_adjust}) is used by an optimizer to adjust the weights such that the loss is minimized with respect to the weights (gradient descent).

\citet{Ronneberger2015} uses the stochastic gradient descent with momentum optimizer implemented in the Machine Learning library Caffe \citep{Jia2014}, where the optimizer is defined as follows,

\begin{equation}
    \label{eq:msgd}
    w_l^{t+1} = \gamma(w_l^t - w_l^{t-1}) - \mu\frac{\partial L}{\partial w_l^t}
\end{equation}

In equation (\ref{eq:msgd}), the superscript $t$ was added to $w$ and refers to training step, which is defined as a prediction and subsequent backpropagation of a batch of samples, where the size of a batch is a pre-determined hyperparameter. $\gamma$ and $\mu$ are the momentum and learning rate hyperparameters respectively. Note that $\gamma$ is introduced by momentum stochastic gradient descent, whereas the learning rate $\mu$ is a hyperparameter common for all deep learning models and determines the rate of weight adjustment as seen in equation (\ref{eq:msgd}).

When all training samples have been inspected once by the U-Net, the training data is shuffled and the above outlined training procedure is repeated. The process of going through all the training data once is defined as an epoch. The number of epochs is a hyperparameter which can be adjusted, and is tied to the bias-variance tradeoff dilemma \citep{Geman1992}. Moreover, the number of epochs determines the duration of training time, and is influenced by the available computing resources.

\citet{Geman1992} states that the cost of low bias in a model is high variance. A model with high bias and low variance is assumed to not have underwent much, if any training, and is thus underfitted to the data. Consequently, a model with low bias but high variance has been trained for a high number of epochs, and is overfitted towards the training-data. An overfitted model is, due to its high variance, ideal at explaining the training data, but lacks the ability to generalize to external datasets. For the training procedure described above, the optimum model has been trained for a sufficient amount of epochs, where it is neither underfitted nor overfitted.

\subsection{Forecast verification metrics}
Verification schemes provide insight into how a forecasting system performs. For this thesis, verification metrics serve a dual purpose. From a model development point of view, verification metrics will be used to increase the skill of the model. However, the same metrics will also be utilized to assess the quality of a prediction as well as explain the physical interpretation of the model \citep{Casati2008}. The model developed for this thesis predicts a scene consisting of labelled pixels, as described in section (\ref{sec:image-segmentation}). It was mentioned in section (\ref{sec:introduction}) that the developed model is aimed towards operational end users, which is partly achieved by validating the model against metrics of end user relevance. Furthermore, it can be assumed that the model and target observations will not differ much outside of the marginal ice zone \citep{Fritzner2020}. Thus, this section will introduce metrics which are relevant for evaluating the sea-ice edge position, as the sea ice edge is important information for maritime operators in the Arctic \citep{Melsom2019}. The following subsections will describe how to determine the position of the sea ice edge, as well as its length according to \citet{Melsom2019}, and derive the Integrated Ice Edge Error \citep{Goessling2016}, with regards to a spatially gridded dataset of deterministic sea ice concentration values.

The Integrated Ice Edge Error is chosen among similar sea ice edge metrics \citep{Melsom2019, Dukhovskoy2015} as it has been shown to be less sensitive to isolated ice patches \citep{Palerme2019}. Furthermore, the work of \citet{Melsom2019} recommends the Integrated Ice Edge Error amongst other metrics for its intuitive interpretation as well as for the possibility to provide the spatial distribution of IIEE areas.

%\subsection{Root Mean Square Error}
% The Root mean square error (rmse) is a commonly used metric which measures the difference between two points in a sample. \citet{Dukhovskoy2015} presents the rmse for sea ice applications as a measure of the pixelwise difference between model data and control data. Moreover, \citet{Dukhovskoy2015} defines rmse in two dimensions as follows,

% \begin{equation}
%     \label{eq:rmse}
%     D_\text{RMSE}(A, B) = \sqrt{\frac{\sum_{i=1}^n\left[(a_i,b_i)\right]^2}{n}}
% \end{equation}

% In equation (\ref{eq:rmse}), A and B refer to the model data and control data sets containing $n$ samples. a, b $\subset$ A, B subscripted by $i$. 

\subsubsection{Defining the Sea Ice Edge}
\label{sec:iceedgelength}
The sea ice edge for a given spatial distribution of sea ice concentration values is derived on a per pixel basis. Let $C \in{\mathbb{R}^2}$ be gridded sea ice concentration values. Then, the sea ice edge is defined as the entries in $C$ which meets the following condition,

\begin{equation}
    \label{eq:iceedge}
    c_{i,j} \geq c_e \wedge \text{min}{(c_{i-1,j},c_{i+1,j},c_{i,j-1},c_{i,j+1})} < c_e
\end{equation}

In Condition (\ref{eq:iceedge}), $c \subset C$ are sea ice concentration values, with $i,j$ denoting indexes. $c_e$ is a given concentration threshold.

Next, let $E \in{\mathbb{R}^2}$ be the set containing sea ice concentration pixels constituting the sea ice edge. It can be seen that the entries $c_{i,j}$ which adhere to condition (\ref{eq:iceedge}) form the set $E$ \citep{Melsom2019}.

Moreover, all the entries in $E$ each contribute to the total length of the sea ice edge, with each entries' length contribution determined based on that entries' 4-connected adjacent grid points, (see figure \ref{fig:4-connection}). Using this formulation, the different combination of neighborhoods in $E$ can result in three different length contributions. For the following contributions, $s$ is the spatial resolution of the grid.

\begin{figure}
    \centering
    \includestandalone[width=.3\textwidth]{/home/arefk/uio/MScThesis_AreKvanum2022_SeaIceML/thesis/methodology/figures/4-connection}
    \caption{\label{fig:4-connection}The gray pixels forms the 4-connected neighborhood of adjacent grid cells for the center pixel.}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width = .5\textwidth]{MelsomIEL.pdf}
    \caption{\label{fig:iceedgeschematic}Sketch of an example gridded ice edge. The gray cells denote ice edge cells, which are labelled and illustrates the ice edge contained in the cell. The black cells denote land. Figure fetched from \protect\citep{Melsom2019}}
\end{figure}

\begin{itemize}
    \item A neighborless pixel is assumed to yield a contribution equal to the length of the diagonal of a grid cell ($l = \sqrt2s$). Here it is assumed that the grid cell only have diagonal neighbors ($\text{e}_\text{a}$ in figure \ref{fig:iceedgeschematic}).
    \item A pixel with one of the four possible adjacent grid points contributes with the mean value between the length of the grid cell and length og the diagonal of the grid cell $l = \frac{s + \sqrt2s}{2}$. It is assumed that the grid cell also has a diagonal neighbor ($\text{e}_\text{b}$ and $\text{e}_\text{e}$ in figure \ref{fig:iceedgeschematic}).
    \item A pixel with two or more of the four adjacent grid points contributes with its spatial resolution (length of the grid cell) $l = s$ ($\text{e}_\text{c}$ and $\text{e}_\text{d}$ in figure \ref{fig:iceedgeschematic}).
\end{itemize}

The final length of the sea ice edge length then becomes

\begin{equation}
    L = \sum_\text{e in E} l^e
\end{equation}

where the superscript $l^e$ denotes the length associated with the entry $e$ according to the algorithm listed above, I.e. the sum of all contributions.

\subsubsection{Integrated Ice Edge Error}
\label{sec:iiee}
The IIEE is an error metric which compares a forecast $f$ to a predefined ground truth target $t$ \citet{Goessling2016}. The metric is defined as

\begin{equation}
    \label{eq:IIEE}
    \text{IIEE} = \text{O} + \text{U}
\end{equation}

where 

\begin{equation}
    \label{eq:a_plus}
    \text{O} = \int_A\text{max}(C_f - C_t, 0)dA
\end{equation}

and

\begin{equation}
    \label{eq:a_minus}
    \text{U} = \int_A\text{max}(C_t - C_f, 0)dA
\end{equation}

with $A \in{\mathbb{R}^2}$ being the area of interest, and is of similar size as $C$. Subscript $f,t$ denotes whether $C$ contains forecasted or target sea ice concentration values. In Equations \ref{eq:a_plus} and \ref{eq:a_minus}, C is binary and is equal to 1 if its concentration value is above a predefined threshold, and 0 elsewhere \citep{Goessling2016}. From the definition of the metric, it can be seen that the IIEE is a sum of the forecast overshoot and undershoot compared to the ground truth target. For a graphical description, see figure (\ref{fig:goessling_iiee})

\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{goessling_iiee}
    \caption{\label{fig:goessling_iiee}15\% sea ice concentration contours for a forecast (blue) and target (red) sea ice concentration product. The IIEE is the sum of the overestimated (O, blue) and underestimated (U, red). White denotes the union between the products. Figure fetched from \protect\citep{Goessling2016}.}
\end{figure}

Additionally, the IIEE can also be represented as a spatial metric by removing the integral with respect to $A$ in equation (\ref{eq:a_plus} and \ref{eq:a_minus}). In this way, the metric is used to define the set of pixels which constitutes its area. To clearly distinguish between the area O (overestimation) and the set of pixels used to compute O, $A^+$ will be used to note the latter. Similarly, $A^-$ will represent the set of pixels constituting U (underestimation). Finally, it can be seen that $A^+$ and $A^-$ represent the spatial distribution of False Positive and False Negatives of the forecast respectively.

The length of the ice edge has a strong influence on the IIEE \citep{Goessling2018,Palerme2019}. Hence, to ensure that forecast errors are comparable across seasons, IIEE is normalized with the length of the ice edge, as mentioned in section (\ref{sec:osisaf}). Furthermore, the normalized IIEE provides an estimate of the displacement error between the forecasted and target sea ice edge \citep{Melsom2019}.

\biblio

\end{document}